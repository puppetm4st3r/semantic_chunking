Capítulo 1 
NUESTRA IMAGEN DEL 
UNIVERSO 
Un conocido científico (algunos dicen que fue Bertrand Russell) daba una vez una  conferencia sobre astronomía. En ella describía cómo la Tierra giraba alrededor del  Sol y cómo éste, a su vez, giraba alrededor del centro de una vasta colección de  estrellas conocida como nuestra galaxia. Al final de la charla, una simpática señora  ya de edad se levantó y le dijo desde el fondo de la sala: «Lo que nos ha contado  usted no son más que tonterías. El mundo es en realidad una plataforma plana  sustentada por el caparazón de una tortuga gigante». El científico sonrió ampliamente antes de replicarle, «¿y en qué se apoya la tortuga?». «Usted es muy inteligente, joven, muy inteligente -dijo la señora-. ¡Pero hay infinitas tortugas una  debajo de otra!». 
La mayor parte de la gente encontraría bastante ridícula la Imagen de nuestro  universo como una torre infinita de tortugas, pero ¿,en qué nos basamos para creer  que lo conocemos mejor? ¿.Qué sabemos acerca del universo, y cómo hemos llegado a saberlo. ¿De dónde surgió el universo, y a dónde va? ¿Tuvo el univer so un  principio, y, si así fue, que sucedió con anterioridad a él? ¿Cuál es la naturaleza del  tiempo? ¿Llegará éste alguna vez a un final? Avances recientes de la física, posibles  en parte gracias a fantásticas nuevas tecnologías, sugieren respuestas a algunas de  estas preguntas que desde hace mucho tiempo nos preocupan. Algún día estas  respuestas podrán parecernos tan obvias como el que la Tierra gire alrededor del  Sol, o, quizás, tan ridículas como una torre de tortugas. Sólo el tiempo (cualquiera  que sea su significado) lo dirá. 
Ya en el año 340 a.C. el filósofo griego Aristóteles, en su libro De los Cielos, fue  capaz de establecer dos buenos argumentos para creer que la Tierra era una esfera  redonda en vez de una plataforma plana. En primer lugar, se dio cuenta de que los  eclipses lunares eran debidos a que la Tierra se situaba entre el Sol y la Luna. La  sombra de la Tierra sobre la Luna era siempre redonda. Si la Tierra hubiera sido un  disco plano, su sombra habría sido alargada y elíptica a menos que el eclipse  siempre ocurriera en el momento en que el Sol estuviera directamente debajo del  centro del disco. En segundo lugar, los griegos sabían, debido a sus viajes, que la  estrella Polar aparecía más baja en el cielo cuando se observaba desde el sur que  cuando se hacía desde regiones más al norte. (Como la estrella Polar está sobre el  polo norte, parecería estar justo encima de un observador situado en dicho polo,  mientras que para alguien que mirara desde el ecuador parecería estar justo en el horizonte.) A partir de la diferencia en la posición aparente de la estrella Polar entre  Egipto y Grecia, Aristóteles incluso estimó que la distancia alrededor de la Tierra era  de 400.000 estadios. No se conoce con exactitud cuál era la longitud de un estadio,  pero puede que fuese de unos 200 metros, lo que supondría que la estimación de  Aristóteles era aproximadamente el doble de la longitud hoy en día aceptada. Los  griegos tenían incluso un tercer argumento en favor de que la Tierra debía de ser  redonda, ¿por qué, si no, ve uno primero las velas de un barco que se acerca en el  horizonte, y sólo después se ve el casco? 

Aristóteles creía que la Tierra era estacionaria y que el Sol, la luna, los planetas y las  estrellas se movían en órbitas circulares alrededor de ella. Creía eso porque estaba  convencido, por razones místicas, de que la Tierra era el centro del universo y de que  el movimiento circular era el más perfecto. Esta idea fue ampliada por Ptolomeo en  el siglo ii d.C. hasta constituir un modelo cosmológico completo. La Tierra permaneció en el centro, rodeada por ocho esferas que transportaban a la Luna, el 

Sol, las estrellas y los cinco planetas conocidos en aquel tiempo, Mercurio, Venus,  Marte, Júpiter y Saturno (figura 1. l). Los planetas se movían en círculos más  pequeños engarzados en sus respectivas esferas para que así se pudieran explicar  sus relativamente complicadas trayectorias celestes. La esfera más externa transportaba a las llamadas estrellas fijas, las cuales siempre permanecían en las  mismas posiciones relativas, las unas con respecto de las otras, girando juntas a  través del cielo. Lo que había detrás de la última esfera nunca fue descrito con  claridad, pero ciertamente no era parte del universo observable por el hombre. 
El modelo de Ptolomeo proporcionaba un sistema razonablemente preciso para  predecir las posiciones de los cuerpos celestes en el firmamento. Pero, para poder  predecir dichas posiciones correctamente, Ptolomeo tenía que suponer que la Luna  seguía un camino que la situaba en algunos instantes dos veces más cerca de la  Tierra que en otros. ¡Y esto significaba que la Luna debería aparecer a veces con  tamaño doble del que usualmente tiene! Ptolomeo reconocía esta inconsistencia, a  pesar de lo cual su modelo fue amplia, aunque no universalmente, aceptado. Fue  adoptado por la Iglesia cristiana como la imagen del universo que estaba de acuerdo  con las Escrituras, y que, además, presentaba la gran ventaja de dejar, fuera de la  esfera de las estrellas fijas, una enorme cantidad de espacio para el cielo y el  infierno. 
Un modelo más simple, sin embargo, fue propuesto, en 1514, por un cura polaco,  Nicolás Copérnico. (Al principio, quizás por miedo a ser tildado de hereje por su  propia iglesia, Copérnico hizo circular su modelo de forma anónima.) Su idea era  que el Sol estaba estacionario en el centro y que la Tierra y los planetas se movían  en órbitas circulares a su alrededor. Pasó casi un siglo antes de que su idea fuera  tomada verdaderamente en serio. Entonces dos astrónomos, el alemán Johannes  Kepler y el italiano Galileo Galilei, empezaron a apoyar públicamente la teoría  copernicana, a pesar de que las órbitas que predecía no se ajustaban fielmente a las  observadas. El golpe mortal a la teoría aristotélico/ptolemaica llegó en 1609. En  ese año, Galileo comenzó a observar el cielo nocturno con un telescopio, que  acababa de inventar. Cuando miró al planeta Júpiter, Galileo encontró que éste  estaba acompañado por varios pequeños satélites o lunas que giraban a su alrededor. Esto implicaba que no todo tenia que girar directamente alrededor de la  Tierra, como Aristóteles y Ptolomeo habían supuesto. (Aún era posible, desde luego,  creer que las lunas de Júpiter se movían en caminos extremadamente complicados  alrededor de la T'ierra, aunque daban la impresión de girar en torno a Júpiter. Sin  embargo, la teoría de Copérnico era mucho más simple.) Al mismo tiempo, Johannes Kepler había modificado la teoría de Copérnico, sugiriendo que los planetas no se movían en círculos, sino en elipses (una elipse es un círculo alargado). Las predicciones se ajustaban ahora finalmente a las observaciones.

Desde el punto de vista de Kepler, las órbitas elípticas constituían meramente una  hipótesis ad hoc, y, de hecho, una hipótesis bastante desagradable, ya que las  elipses eran claramente menos perfectas que los círculos. Kepler, al descubrir casi  por accidente que las órbitas elípticas se ajustaban bien a las observaciones, no  pudo reconciliarlas con su idea de que los planetas estaban concebidos para girar  alrededor del Sol atraídos por fuerzas magnéticas. Una explicación coherente sólo  fue proporcionada mucho más tarde, en 1687, cuando sir Isaac Newton publicó su  Philosophiae Naturalis Principia Mathematica, probablemente la obra más importante publicada en las ciencias físicas en todos los tiempos. En ella, Newton  no sólo presentó una teoría de cómo se mueven los cuerpos en el espacio y en el  tiempo, sino que también desarrolló las complicadas matemáticas necesarias para  analizar esos movimientos. Además, Newton postuló una ley de la gravitación universal, de acuerdo con la cual cada cuerpo en el universo era atraído por cualquier  otro cuerpo con una fuerza que era tanto mayor cuanto más masivos fueran los  cuerpos y cuanto más cerca estuvieran el uno del otro. Era esta misma fuerza la que  hacía que los objetos cayeran al suelo. (La historia de que Newton fue inspirado por  una manzana que cayó sobre su cabeza es casi seguro apócrifa. Todo lo que  Newton mismo llegó a decir fue que la idea de la gravedad le vino cuando estaba  sentado «en disposición contemplativa», de la que «únicamente le distrajo la caída  de una manzana».) Newton pasó luego a mostrar que, de acuerdo con su ley, la  gravedad es la causa de que la Luna se mueva en una órbita elíptica alrededor de la  Tierra, y de que la Tierra y los planetas sigan caminos elípticos alrededor del Sol. 
El modelo copernicano se despojó de las esferas celestiales de Ptolomeo y, con  ellas, de la idea de que el universo tiene una frontera natural. Ya que las «estrellas  fijas» no parecían cambiar sus posiciones, aparte de una rotación a través del cielo  causada por el giro de la Tierra sobre su eje, llegó a ser natural suponer que las  estrellas fijas eran objetos como nuestro Sol, pero mucho más lejanos. 
Newton comprendió que, de acuerdo con su teoría de la gravedad, las estrellas  deberían atraerse unas a otras, de forma que no parecía posible que pudieran  permanecer esencialmente en reposo. ¿No llegaría un determinado momento en el  que todas ellas se aglutinarían? En 1691, en una carta a Richard Bentley, otro  destacado pensador de su época, Newton argumentaba que esto verdaderamente  sucedería si sólo hubiera un número finito de estrellas distribuidas en una región  finita del espacio. Pero razonaba que si, por el contrario, hubiera un número infinito  de estrellas, distribuidas más o menos uniformemente sobre un espacio infinito, ello  no sucedería, porque no habría ningún punto central donde aglutinarse.

Este argumento es un ejemplo del tipo de dificultad que uno puede encontrar cuando  se discute acerca del infinito. En un universo infinito, cada punto puede ser considerado como el centro, ya que todo punto tiene un número infinito de estrellas a  cada lado. La aproximación correcta, que sólo fue descubierta mucho más tarde, es  considerar primero una situación finita, en la que las estrellas tenderían a aglutinarse,  y preguntarse después cómo cambia la situación cuando uno añade más estrellas  uniformemente distribuidas fuera de la región considerada. De acuerdo con la ley de  Newton, las estrellas extra no producirían, en general, ningún cambio sobre las estrellas originales, que por lo tanto continuarían aglutinándose con la misma rapidez. Podemos añadir tantas estrellas como queramos, que a pesar de ello las  estrellas originales seguirán juntándose indefinidamente. Esto nos asegura que es  imposible tener un modelo estático e infinito del universo, en el que la gravedad sea  siempre atractiva. 
Un dato interesante sobre la corriente general del pensamiento anterior al siglo xx es  que nadie hubiera sugerido que el universo se estuviera expandiendo o contrayendo.  Era generalmente aceptado que el universo, o bien había existido por siempre en un  estado inmóvil, o bien había sido creado, más o menos como lo observamos hoy, en  un determinado tiempo pasado finito. En parte, esto puede deberse a la tendencia  que tenemos las personas a creer en verdades eternas, tanto como al consuelo que  nos proporciona la creencia de que, aunque podamos envejecer y morir, el universo  permanece eterno e inmóvil. 
Incluso aquellos que comprendieron que la teoría de la gravedad de Newton mostraba que el universo no podía ser estático, no pensaron en sugerir que podría  estar expandiéndose. Por el contrario, intentaron modificar la teoría suponiendo que  la fuerza gravitacional fuese repulsiva a distancias muy grandes. Ello no afectaba  significativamente a sus predicciones sobre el movimiento de los planetas, pero  permitía que una distribución infinita de estrellas pudiera permanecer en equilibrio,  con las fuerzas atractivas entre estrellas cercanas equilibradas por las fuerzas repulsivas entre estrellas lejanas. Sin embargo, hoy en día creemos que tal equilibrio  sería inestable: si las estrellas en alguna región se acercaran sólo ligeramente unas  a otras, las fuerzas atractivas entre ellas se harían más fuertes y dominarían sobre  las fuerzas repulsivas, de forma que las estrellas, una vez que empezaran a aglutinarse, lo seguirían haciendo por siempre. Por el contrario, si las estrellas  empezaran a separarse un poco entre sí, las fuerzas repulsivas dominarían alejando  indefinidamente a unas estrellas de otras. 
Otra objeción a un universo estático infinito es normalmente atribuida al filósofo  alemán Heinrich Olbers, quien escribió acerca de dicho modelo en 1823. En realidad, varios contemporáneos de Newton habían considerado ya el problema, y el artículo de Olbers no fue ni siquiera el primero en contener argumentos plausibles en  contra del anterior modelo. Fue, sin embargo, el primero en ser ampliamente  conocido. La dificultad a la que nos referíamos estriba en que, en un universo  estático infinito, prácticamente cada línea de visión acabaría en la superficie de una  estrella. Así, sería de esperar que todo el cielo fuera, incluso de noche, tan brillante  como el Sol. El contraargumento de Olbers era que la luz de las estrellas lejanas  estaría oscurecida por la absorción debida a la materia intermedia. Sin embargo, si  eso sucediera, la materia intermedia se calentaría, con el tiempo, hasta que iluminara de forma tan brillante como las estrellas. La única manera de evitar la  conclusión de que todo el cielo nocturno debería de ser tan brillante como la superficie del Sol sería suponer que las estrellas no han estado iluminando desde  siempre, sino que se encendieron en un determinado instante pasado finito. En este  caso, la materia absorbente podría no estar caliente todavía, o la luz de las estrellas  distantes podría no habernos alcanzado aún. Y esto nos conduciría a la cuestión de  qué podría haber causado el hecho de que las estrellas se hubieran encendido por  primera vez. 
El principio del universo había sido discutido, desde luego, mucho antes de esto. De  acuerdo con distintas cosmologías primitivas y con la tradición judeo-cristiana musulmana, el universo comenzó en cierto tiempo pasado finito, y no muy distante.  Un argumento en favor de un origen tal fue la sensación de que era necesario tener  una «Causa Primera» para explicar la existencia del universo. (Dentro del universo,  uno siempre explica un acontecimiento como causado por algún otro acontecimiento  anterior, pero la existencia del universo en sí, sólo podría ser explicada de esta  manera si tuviera un origen.) Otro argumento lo dio san Agustín en su libro La ciudad  de Dios. Señalaba que la civilización está progresando y que podemos recordar  quién realizó esta hazaña o desarrolló aquella técnica. Así, el hombre, y por lo tanto  quizás también el universo, no podía haber existido desde mucho tiempo atrás. San  Agustín, de acuerdo con el libro del Génesis, aceptaba una fecha de unos 5.000  años antes de Cristo para la creación del universo. (Es interesante comprobar que  esta fecha no está muy lejos del final del último periodo glacial, sobre el 10.000 a.C.,  que es cuando los arqueólogos suponen que realmente empezó la civilización.) 
Aristóteles, y la mayor parte del resto de los filósofos griegos, no era partidario, por  el contrario, de la idea de la creación, porque sonaba demasiado a intervención  divina. Ellos creían, por consiguiente, que la raza humana y el mundo que la rodea  habían existido, y existirían, por siempre. Los antiguos ya habían considerado el  argumento descrito arriba acerca del progreso, y lo habían resuelto diciendo que  había habido inundaciones periódicas u otros desastres que repetidamente situaban  a la raza humana en el principio de la civilización.

Las cuestiones de si el universo tiene un principio en el tiempo y de si está limitado  en el espacio fueron posteriormente examinadas de forma extensiva por el filósofo  Immanuel Kant en su monumental (y muy oscura) obra, Crítica de la razón pura,  publicada en 1781. Él llamó a estas cuestiones antinomias (es decir, contradicciones) de la razón pura, porque le parecía que había argumentos igualmente convincentes para creer tanto en la tesis, que el universo tiene un principio, como en la antítesis, que el universo siempre había existido. Su argumento  en favor de la tesis era que si el universo no hubiera tenido un principio, habría  habido un período de tiempo infinito anterior a cualquier acontecimiento, lo que él  consideraba absurdo. El argumento en pro de la antítesis era que si el universo  hubiera tenido un principio, habría habido un período de tiempo infinito anterior a él, y  de este modo, ¿por qué habría de empezar el universo en un tiempo particular  cualquiera? De hecho, sus razonamientos en favor de la tesis y de la antítesis son  realmente el mismo argumento. Ambos están basados en la suposición implícita de que el tiempo continúa hacia atrás indefinidamente, tanto si el universo ha existido  desde siempre como si no. Como veremos, el concepto de tiempo no tiene  significado antes del comienzo del universo. Esto ya había sido señalado en primer  lugar por san Agustín. Cuando se le preguntó: ¿Qué hacía Dios antes de que creara  el universo?, Agustín no respondió: estaba preparando el infierno para aquellos que  preguntaran tales cuestiones. En su lugar, dijo que el tiempo era una propiedad del  universo que Dios había creado, y que el tiempo no existía con anterioridad al  principio del universo. 
Cuando la mayor parte de la gente creía en un universo esencialmente estático e  inmóvil, la pregunta de si éste tenía, o no, un principio era realmente una cuestión de  carácter metafísico o teológico. Se podían explicar igualmente bien todas las observaciones tanto con la teoría de que el universo siempre había existido, como  con la teoría de que había sido puesto en funcionamiento en un determinado tiempo  finito, de tal forma que pareciera como si hubiera existido desde siempre. Pero, en  1929, Edwin Hubble hizo la observación crucial de que, donde quiera que uno mire,  las galaxias distantes se están alejando de nosotros. O en otras palabras, el universo  se está expandiendo. Esto significa que en épocas anteriores los objetos deberían  de haber estado más juntos entre sí. De hecho, parece ser que hubo un tiempo,  hace unos diez o veinte mil millones de años, en que todos los objetos estaban en el  mismo lugar exactamente, y en el que, por lo tanto, la densidad del universo era  infinita. Fue dicho descubrimiento el que finalmente llevó la cuestión del principio del  universo a los dominios de la ciencia. 
Las observaciones de Hubble sugerían que hubo un tiempo, llamado el big bang  [gran explosión o explosión primordial], en que el universo era infinitésimamente  pequeño e infinitamente denso. Bajo tales condiciones, todas las leyes de la ciencia, y, por tanto, toda capacidad de predicción del futuro, se desmoronarían. Si  hubiera habido acontecimientos anteriores a este no podrían afectar de ninguna  manera a lo que ocurre en el presente. Su existencia podría ser ignorada, ya que  ello no extrañaría consecuencias observables. Uno podría decir que el tiempo tiene  su origen en el big bang, en el sentido de que los tiempos anteriores simplemente no  estarían definidos. Es señalar que este principio del tiempo es radicalmente diferente de aquellos previamente considerados. En un universo inmóvil, un principio  del tiempo es algo que ha de ser impuesto por un ser externo al universo; no existe la  necesidad de un principio. Uno puede imaginarse que Dios creó el universo en,  textualmente, cualquier instante de tiempo. Por el contrario, si el universo se está  expandiendo, pueden existir poderosas razones físicas para que tenga que haber un  principio. Uno aún se podría imaginar que Dios creó el universo en el instante del big  bang, pero no tendría sentido suponer que el universo hubiese sido creado antes del  big bang. ¡Universo en expansión no excluye la existencia de un creador, pero sí  establece límites sobre cuándo éste pudo haber llevado a cabo su misión! 
Para poder analizar la naturaleza del universo, y poder discutir cuestiones tales como  si ha habido un principio o si habrá un final, es necesario tener claro lo que es una  teoría científica. Consideremos aquí un punto de vista ingenuo, en el que una teoría  es simplemente un modelo del universo, o de una parte de él, y un conjunto de reglas  que relacionan las magnitudes del modelo con las observaciones que realizamos.  Esto sólo existe en nuestras mentes, y no tiene ninguna otra realidad (cualquiera que  sea lo que esto pueda significar). Una teoría es una buena teoría siempre que  satisfaga dos requisitos: debe describir con precisión un amplio conjunto de observaciones sobre la base de un modelo que contenga sólo unos pocos parámetros arbitrarios, y debe ser capaz de predecir positivamente los resultados de  observaciones futuras. Por ejemplo, la teoría de Aristóteles de que todo estaba  constituido por cuatro elementos, tierra, aire, fuego y agua, era lo suficientemente  simple como para ser cualificada como tal, pero fallaba en que no realizaba ninguna  predicción concreta. Por el contrario, la teoría de la gravedad de Newton estaba  basada en un modelo incluso más simple, en el que los cuerpos se atraían entre sí  con una fuerza proporcional a una cantidad llamada masa e inversamente proporcional al cuadrado de la distancia entre ellos, a pesar de lo cual era capaz de  predecir el movimiento del Sol, la Luna y los planetas con un alto grado de precisión. 
Cualquier teoría física es siempre provisional, en el sentido de que es sólo una  hipótesis: nunca se puede probar. A pesar de que los resultados de los experimentos concuerden muchas veces con la teoría, nunca podremos estar seguros de que la próxima vez el resultado no vaya a contradecirla. Sin embargo, se  puede rechazar una teoría en cuanto se encuentre una única observación que  contradiga sus predicciones. Como ha subrayado el filósofo de la ciencia Karl 

Popper, una buena teoría está caracterizada por el hecho de predecir un gran número de resultados que en principio pueden ser refutados o invalidados por la  observación. Cada vez que se comprueba que un nuevo experimento está de  acuerdo con las predicciones, la teoría sobrevive y nuestra confianza en ella aumenta. Pero si por el contrario se realiza alguna vez una nueva observación que  contradiga la teoría, tendremos que abandonarla o modificarla. 0 al menos esto es lo  que se supone que debe suceder, aunque uno siempre puede cuestionar la competencia de la persona que realizó la observación. 
En la práctica, lo que sucede es que se construye una nueva teoría que en realidad  es una extensión de la teoría original. Por ejemplo, observaciones tremendamente  precisas del planeta Mercurio revelan una pequeña diferencia entre su movimiento y  las predicciones de la teoría de la gravedad de Newton. La teoría de la relatividad  general de Einstein predecía un movimiento de Mercurio ligeramente distinto del de  la teoría de Newton. El hecho de que las predicciones de Einstein se ajustaran a las  observaciones, mientras que las de Newton no lo hacían, fue una de las confirmaciones cruciales de la nueva teoría. Sin embargo, seguimos usando la teoría  de Newton para todos los propósitos prácticos ya que las diferencias entre sus  predicciones y las de la relatividad general son muy pequeñas en las situaciones que  normalmente nos incumben. (¡La teoría de Newton también posee la gran ventaja de  ser mucho más simple y manejable que la de Einstein!) 
El objetivo final de la ciencia es el proporcionar una única que describa correctamente todo el universo. Sin embargo, el método que la mayoría de los  científicos siguen en realidad es el de separar el problema en dos partes. Primero,  están las leyes que nos dicen cómo cambia el universo con el tiempo. (Si conocemos cómo es el universo en un instante dado, estas leves físicas nos dirán  cómo será el universo en cualquier otro posterior.) Segundo, está la cuestión del  estado inicial del universo. Algunas personas creen que la ciencia se debería  ocupar únicamente de la primera parte: consideran el tema de la situación inicial del  universo como objeto de la metafísica o la religión. Ellos argumentarían que Dios, al  ser omnipotente, podría haber iniciado el universo de la manera que más le hubiera  gustado. Puede ser que sí, pero en ese caso él también haberlo hecho evolucionar  de un modo totalmente arbitrario. En cambio, parece ser que eligió hacerlo evolucionar de una manera muy regular siguiendo ciertas leyes. Resulta, así pues,  igualmente razonable suponer que también hay leyes que gobiernan el estado inicial. 
Es muy difícil construir una única teoría capaz de describir todo el universo. En vez  de ello, nos vemos forzados, de momento, a dividir el problema en varias partes,  inventando un cierto número de teorías parciales. Cada una de estas teorías parciales describe y predice una cierta clase restringida de observaciones,despreciando los efectos de otras cantidades, o representando éstas por simples  conjuntos de números. Puede ocurrir que esta aproximación sea completamente  errónea. Si todo en el universo depende de absolutamente todo el resto de él de una  manera fundamental, podría resultar imposible acercarse a una solución completa  investigando partes aisladas del problema. Sin embargo, este es ciertamente el  modo en que hemos progresado en el pasado. El ejemplo clásico es de nuevo la  teoría de la gravedad de Newton, la cual nos dice que la fuerza gravitacional entre  dos cuerpos depende únicamente de un número asociado a cada cuerpo, su masa,  siendo por lo demás independiente del tipo de sustancia que forma el cuerpo. Así,  no se necesita tener una teoría de la estructura y constitución del Sol y los planetas  para poder determinar sus órbitas. 
Los científicos actuales describen el universo a través de dos teorías parciales fundamentales: la teoría de la relatividad general y la mecánica cuántica. Ellas  constituyen el gran logro intelectual de la primera mitad de este siglo. La teoría de la  relatividad general describe la fuerza de la gravedad y la estructura a gran escala del  universo, es decir, la estructura a escalas que van desde sólo unos pocos kilómetros  hasta un billón de billones (un 1 con veinticuatro ceros detrás) de kilómetros, el  tamaño del universo observable. La mecánica cuántica, por el contrario, se ocupa  de los fenómenos a escalas extremadamente pequeñas, tales como una billonésima  de centímetro. Desafortunadamente, sin embargo, se sabe que estas dos teorías  son inconsistentes entre sí: ambas no pueden ser correctas a la vez. Uno de los  mayores esfuerzos de la física actual, y el tema principal de este libro, es la búsqueda de una nueva teoria que incorpore a las dos anteriores: una teoría cuántica de la gravedad. Aún no se dispone de tal teoría, y para ello todavía puede  quedar un largo camino por recorrer, pero sí se conocen muchas de las propiedades  que debe poseer. En capítulos posteriores veremos que ya se sabe relativamente  bastante acerca de las predicciones que debe hacer una teoría cuántica de la  gravedad. 
Si se admite entonces que el universo no es arbitrario, sino que está gobernado por  ciertas leyes bien definidas, habrá que combinar al final las teorías parciales en una  teoría unificada completa que describirá todos los fenómenos del universo. Existe,  no obstante, una paradoja fundamental en nuestra búsqueda de esta teoría unificada  completa. Las ideas anteriormente perfiladas sobre las teorías científicas suponen  que somos seres racionales, libres para observar el universo como nos plazca y para  extraer deducciones lógicas de lo que veamos. En tal esquema parece razonable  suponer que podríamos continuar progresando indefinidamente, acercándonos cada  vez más a las leyes que gobiernan el universo. Pero si realmente existiera una teoría  unificada completa, ésta también determinaría presumiblemente nuestras acciones.  ¡Así la teoría misma determinaría el resultado de nuestra búsqueda de ella! ¿Y por qué razón debería determinar que llegáramos a las verdaderas conclusiones a partir  de la evidencia que nos presenta? ¿Es que no podría determinar igualmente bien  que extrajéramos conclusiones erroneas? ¿O incluso que no extrajéramos ninguna  conclusión en absoluto? 
La única respuesta que puedo dar a este problema se basa en el principio de la  selección natural de Darwin. La idea estriba en que en cualquier población de  organismos autorreproductores, habrá variaciones tanto en el material genétíco  como en educación de los diferentes individuos. Estas diferencias supondrán que  algunos individuos sean más capaces que otros para extraer las conclusiones correctas acerca del mundo que rodea, y para actuar de acuerdo con ellas. Dichos  individuos tendrán más posibilidades de sobrevivir y reproducirse, de forma que su  esquema mental y de conducta acabará imponiéndose. En el pasado ha sido cierto  que lo que llamamos inteligencia y descubrimiento científico han supuesto una ventaja en el aspecto de la supervivencia. No es totalmente evidente que esto tenga  que seguir siendo así: nuestros descubrimientos científicos podrían destruirnos a  todos perfectamente, e, incluso si no lo hacen, una teoría unificada completa no tiene  por qué suponer ningún cambio en lo concerniente a nuestras posibilidades de  supervivencia. Sin embargo, dado que el universo ha evolucionado de un modo  regular, podríamos esperar que las capacidades de razonamiento que la selección  natural nos ha dado sigan siendo válidas en nuestra búsqueda de una teoría unificada completa, y no nos conduzcan a conclusiones erróneas. 
Dado que las teorías que ya poseemos son suficientes para realizar predicciones  exactas de todos los fenómenos naturales, excepto de los más extremos, nuestra  búsqueda de la teoría definitiva del universo parece difícil de justificar desde un  punto de vista práctico. (Es interesante señalar, sin embargo, que argumentos similares podrían haberse usado en contra de la teoría de la relatividad y de la  mecánica cuántica, las cuales nos han dado la energía nuclear y la revolución de la  microelectrónica.) Así pues, el descubrimiento de una teoría unificada completa  puede no ayudar a la supervivencia de nuestra especie. Puede incluso no afectar a  nuestro modo de vida. Pero siempre, desde el origen de la civilización, la gente no  se ha contentado con ver los acontecimientos como desconectados e inexplicables.  Ha buscado incesantemente un conocimiento del orden subyacente del mundo. Hoy  en día, aún seguimos anhelando saber por qué estamos aquí y de dónde venimos.  El profundo deseo de conocimiento de la humanidad es justificación suficiente para  continuar nuestra búsqueda. Y ésta no cesará hasta que poseamos una descripción  completa del universo en el que vivimos.

Capítulo 2 
ESPACIO Y TIEMPO 
Nuestras ideas actuales acerca del movimiento de los cuerpos se remontan a  Galileo y Newton. Antes de ellos, se creía en las ideas de Aristóteles, quien decía  que el estado natural de un cuerpo era estar en reposo y que éste sólo se movía si  era empujado por una fuerza o un impulso. De ello se deducía que un cuerpo  pesado debía caer más rápido que uno ligero, porque sufría una atracción mayor  hacia la tierra. 
La tradición aristotélica también mantenía que se podrían deducir todas las leyes  que gobiernan el universo por medio del pensamiento puro: no era necesario  comprobarlas por medio de la observación. Así, nadie antes de Galileo se preocupó  de ver si los cuerpos con pesos diferentes caían con velocidades diferentes. Se  dice que Galileo demostró que las anteriores ideas de Aristóteles eran falsas 
dejando caer diferentes pesos desde la torre inclinada de Pisa. Es casi seguro que  esta historia no es cierta, aunque lo que sí hizo Galileo fue algo equivalente: dejó  caer bolas de distintos pesos a lo largo de un plano inclinado. La situación es muy  similar a la de los cuerpos pesados que caen verticalmente, pero es más fácil de  observar porque las velocidades son menores. Las mediciones de Galileo indicaron  que cada cuerpo aumentaba su velocidad al mismo ritmo, independientemente de su  peso. Por ejemplo, si se suelta una bola en una pendiente que desciende un metro  por cada diez metros de recorrido, la bola caerá por la pendiente con una velocidad  de un metro por segundo después de un segundo, de dos metros por segundo  después de dos segundos, y así sucesivamente, sin importar lo pesada que sea la  bola. Por supuesto que una bola de plomo caerá más rápida que una pluma, pero  ello se debe únicamente a que la pluma es frenada por la resistencia del aire. Si uno  soltara dos cuerpos que no presentasen demasiada resistencia al aire, tales como  dos pesos diferentes de plomo, caerían con la misma rapidez. 
Las mediciones de Galileo sirvieron de base a Newton para la obtención de sus  leyes del movimiento. En los experimentos de Galileo, cuando un cuerpo caía  rodando, siempre actuaba sobre él la misma fuerza (su peso) y el efecto que se  producía consistía en acelerarlo de forma constante. Esto demostraba que el efecto  real de una fuerza era el de cambiar la velocidad del cuerpo, en vez de simplemente  ponerlo en movimiento, como se pensaba anteriormente. Ello también significaba  que siempre que sobre un cuerpo no actuara ninguna fuerza, éste se mantendría  moviéndose en una línea recta con la misma velocidad. Esta idea fue formulada explícitamente por primera vez en los Principia Mathematica de Newton, publicados  en 1687, y se conoce como primera ley de Newton. Lo que le sucede a un cuerpo  cuando sobre él actúa una fuerza está recogido en la segunda ley de Newton. Ésta  afirma que el cuerpo se acelerará, o cambiará sir velocidad, a un ritmo proporcional  a la fuerza. (Por ejemplo, la aceleración se duplicará cuando la fuerza aplicada sea  doble.) Al mismo tiempo, la aceleración disminuirá cuando aumente la masa (o la  cantidad de materia) del cuerpo. (La misma fuerza actuando sobre un cuerpo de  doble masa que otro, producirá la mitad de aceleración en el primero que en el  segundo.) Un ejemplo familiar lo tenemos en un coche: cuanto más potente sea su  motor mayor aceleración poseerá, pero cuanto más pesado sea el coche menor  aceleración tendrá con el mismo motor. 
Además de las leyes del movimiento, Newton descubrió una ley que describía la  fuerza de la gravedad, una ley que nos dice que todo cuerpo atrae a todos los demás  cuerpos con una fuerza proporcional a la masa de cada uno de ellos. Así, la fuerza  entre dos cuerpos se duplicará si uno de ellos (digamos, el cuerpo A) dobla su masa.  Esto es lo que razonablemente se podría esperar, ya que uno puede suponer al  nuevo cuerpo A formado por dos cuerpos, cada uno de ellos con la masa original.  Cada uno de estos cuerpos atraerá al cuerpo B con la fuerza original. Por lo tanto, la  fuerza total entre A y B será justo el doble que la fuerza original. Y si, por ejemplo,  uno de los cuerpos tuviera una masa doble de la original y el otro cuerpo una masa  tres veces mayor que al principio, la fuerza entre ellos sería seis veces más intensa  que la original. Se puede ver ahora por qué todos los cuerpos caen con la misma  rapidez: un cuerpo que tenga doble peso sufrirá una fuerza gravitatoria doble, pero al  mismo tiempo tendrá una masa doble. De acuerdo con la segunda ley de Newton,  estos dos efectos se cancelarán exactamente y la aceleración será la misma en  ambos casos. 
La ley de la gravedad de Newton nos dice también que cuanto más separados estén  los cuerpos menor será la fuerza gravitatoria entre ellos. La ley de la gravedad de  Newton establece que la atracción gravitatoria producida por una estrella a una cierta  distancia es exactamente la cuarta parte de la que produciría una estrella similar a la  mitad de distancia. Esta ley predice con gran precisión las órbitas de la Tierra, la  Luna y los planetas. Si la ley fuera que la atracción gravitatoria de una estrella  decayera más rápidamente con la distancia, las órbitas de los planetas no serían  elípticas, sino que éstos irían cayendo en espiral hacia el Sol. Si, por el contrario, la  atracción gravitatoria decayera más lentamente, las fuerzas gravitatorias debidas a  las estrellas lejanas dominarían frente a la atracción de la Tierra. 
La diferencia fundamental entre las ideas de Aristóteles y las de Galileo y Newton  estriba en que Aristóteles creía en un estado preferente de reposo, en el que todas las cosas subyacerían, a menos que fueran empujadas por una fuerza o impulso. En  particular, él creyó que la Tierra estaba en reposo. Por el contrario, de las leyes de  Newton se desprende que no existe un único estándar de reposo. Se puede  suponer igualmente o que el cuerpo A está en reposo y el cuerpo B se mueve a  velocidad constante con respecto de A, o que el B está en reposo y es el cuerpo A el  que se mueve. Por ejemplo, si uno se olvida de momento de la rotación de la Tierra  y de su órbita alrededor del Sol, se puede decir que la Tierra está en reposo y que un  tren sobre ella está viajando hacia el norte a ciento cuarenta kilómetros por hora, o  se puede decir igualmente que el tren está en reposo y que la Tierra se mueve hacia  el sur a ciento cuarenta kilómetros por hora. Si se realizaran experimentos en el tren  con objetos que se movieran, comprobaríamos que todas las leyes de Newton seguirían siendo válidas. Por ejemplo, al jugar al ping-pong en el tren, uno encontraría que la pelota obedece las leyes de Newton exactamente igual a como lo  haría en una mesa situada junto a la vía. Por lo tanto, no hay forma de distinguir si es  el tren o es la Tierra lo que se mueve. 
La falta de un estándar absoluto de reposo significaba que no se podía determinar si  dos acontecimientos que ocurrieran en tiempos diferentes habían tenido lugar en la  misma posición espacial. Por ejemplo, supongamos que en el tren nuestra bola de  ping-pong está botando, moviéndose verticalmente hacia arriba y hacia abajo y 
golpeando la mesa dos veces en el mismo lugar con un intervalo de un segundo.  Para un observador situado junto a la vía, los dos botes parecerán tener lugar con  una separación de unos cuarenta metros, ya que el tren habrá recorrido esa distancia entre los dos botes. Así pues la no existencia de un reposo absoluto  significa que no se puede asociar una posición absoluta en el espacio con un  suceso, como Aristóteles había creído. Las posiciones de los sucesos y la distancia  
entre ellos serán diferentes para una persona en el tren y para otra que esté al lado  de la vía, y no existe razón para preferir el punto de vista de una de las personas  frente al de la otra. 
Newton estuvo muy preocupado por esta falta de una posición absoluta, o espacio  absoluto, como se le llamaba, porque no concordaba con su idea de un Dios absoluto. De hecho, rehusó aceptar la no existencia de un espacio absoluto, a pesar  incluso de que estaba implicada por sus propias leyes. Fue duramente criticado por  mucha gente debido a esta creencia irracional, destacando sobre todo la crítica del  obispo Berkeley, un filósofo que creía que todos los objetos materiales, junto con el  espacio y el tiempo, eran una ilusión. Cuando el famoso Dr. Johnson se enteró de la  opinión de Berkeley gritó «¡Lo rebato así!» y golpeó con la punta del pie una gran  piedra.

Tanto Aristóteles como Newton creían en el tiempo absoluto. Es decir, ambos pensaban que se podía afirmar inequívocamente la posibilidad de medir el intervalo  de tiempo entre dos sucesos sin ambigüedad, y que dicho intervalo sería el mismo  para todos los que lo midieran, con tal que usaran un buen reloj. El tiempo estaba  totalmente separado y era independiente del espacio. Esto es, de hecho, lo que la  mayoría de la gente consideraría como de sentido común. Sin embargo, hemos  tenido que cambiar nuestras ideas acerca del espacio y del tiempo. Aunque  nuestras nociones de lo que parece ser el sentido común funcionan bien cuando se  usan en el estudio del movimiento de las cosas, tales como manzanas o planetas,  que viajan relativamente lentas, no funcionan, en absoluto, cuando se aplican a cosas  que se mueven con o cerca de la velocidad de la luz. 
El hecho de que la luz viaja a una velocidad finita, aunque muy elevada, fue descubierto en 1676 por el astrónomo danés Ole Christensen Roemer. Él observó  que los tiempos en los que las lunas de Júpiter parecían pasar por detrás de éste no  estaban regularmente espaciados, como sería de esperar si las lunas giraran alrededor de Júpiter con un ritmo constante. Dado que la Tierra y Júpiter giran  alrededor del Sol, la distancia entre ambos varía. Roemer notó que los eclipses de  las lunas de Júpiter parecen ocurrir tanto más tarde cuanto más distantes de Júpiter  estamos. Argumentó que se debía a que la luz proveniente de las lunas tardaba más  en llegar a nosotros cuanto más lejos estábamos de ellas. Sus medidas sobre las  variaciones de las distancias de la Tierra a Júpiter no eran, sin embargo, demasiado  buenas, y así estimó un valor para la velocidad de la luz de 225.000 kilómetros por  segundo, comparado con el valor moderno de 300.000 kilómetros por segundo. No  obstante, no sólo el logro de Roemer de probar que la luz viaja a una velocidad finita,  sino también de medir esa velocidad, fue notable, sobre todo teniendo en cuenta que  esto ocurría once años antes de que Newton publicara los Principia Mathematica. 
Una verdadera teoría de la propagación de la luz no surgió hasta 1865, en que el  físico británico James Clerk Maxwell consiguió unificar con éxito las teorías parciales  que hasta entonces se habían usado para definir las fuerzas de la electricidad y el  magnetismo. Las ecuaciones de Maxwell predecían que podían existir perturbaciones de carácter ondulatorio del campo electromagnético combinado, y  que éstas viajarían a velocidad constante, como las olas de una balsa. Si tales  ondas poseen una longitud de onda (la distancia entre una cresta de onda y la  siguiente) de un metro o más, constituyen lo que hoy en día llamamos ondas de  radio. Aquellas con longitudes de onda menores se llaman microondas (unos pocos  centímetros) o infrarrojas (más de una diezmilésima de centímetro). La luz visible  tiene sólo una longitud de onda de entre cuarenta y ochenta millonésimas de centímetro. Las ondas con todavía menores longitudes se conocen como radiación  ultravioleta, rayos X y rayos gamma.

La teoría de Maxwell predecía que tanto las ondas de radio como las luminosas  deberían viajar a una velocidad fija determinada. La teoría de Newton se había  desprendido, sin embargo, de un sistema de referencia absoluto, de tal forma que si  se suponía que la luz viajaba a una cierta velocidad fija, había que especificar con  respecto a qué sistema de referencia se medía dicha velocidad. Para que esto  tuviera sentido, se sugirió la existencia de una sustancia llamada «éter» que estaba  presente en todas partes, incluso en el espacio «vacío». Las ondas de luz debían  viajar a través del éter al igual que las ondas de sonido lo hacen a través del aire, y  sus velocidades deberían ser, por lo tanto, relativas al éter. Diferentes observadores, que se movieran con relación al éter, verían acercarse la luz con  velocidades distintas, pero la velocidad de la luz con respecto al éter permanecería  fija. En particular, dado que la Tierra se movía a través del éter en su órbita  alrededor del Sol, la velocidad de la luz medida en la dirección del movimiento de la  Tierra a través del éter (cuando nos estuviéramos moviendo hacia la fuente luminosa)  debería ser mayor que la velocidad de la luz en la dirección perpendicular a ese  movimiento (cuando no nos estuviéramos moviendo hacia la fuente). En 1887, Albert  Michelson (quien más tarde fue el primer norteamericano que recibió el premio  Nobel de física) y Edward Morley llevaron a cabo un' muy esmerado experimento en  la Case School of Applied Science, de Cleveland. Ellos compararon la velocidad de  la luz en la dirección del movimiento de la Tierra, con la velocidad de la luz en la  dirección perpendicular a dicho movimiento. Para su sorpresa ¡encontraron que  ambas velocidades eran exactamente iguales! 
Entre 1887 y 1905, hubo diversos intentos, los más importantes debidos al físico  holandés Hendrik Lorentz, de explicar el resultado del experimento de Michelson Morley en términos de contracción de los objetos o de retardo de los relojes cuando  éstos se mueven a través del éter. Sin embargo, en 1905, en un famoso artículo  Albert Einstein, hasta entonces un desconocido empleado de la oficina de patentes  de Suiza, señaló que la idea del éter era totalmente innecesaria, con tal que se  estuviera dispuesto a abandonar la idea de un tiempo absoluto. Una proposición  similar fue realizada unas semanas después por un destacado matemático francés,  Henri Poincaré. Los argumentos de Einstein tenían un carácter más físico que los de  Poincaré, que había estudiado el problema desde un punto de vista puramente  matemático. A Einstein se le reconoce como el creador de la nueva teoría, mientras  que a Poincaré se le recuerda por haber dado su nombre a una parte importante de  la teoría. 
El postulado fundamental de la teoría de la relatividad, nombre de esta nueva teoría,  era que las leyes de la ciencia deberían ser las mismas para todos los observadores  en movimiento libre, independientemente de cual fuera su velocidad. Esto ya era cierto para las leyes de Newton, pero ahora se extendía la idea para incluir también la teoría de Maxwell y la velocidad de la luz: todos los observadores deberían medir  la misma velocidad de la luz sin importar la rapidez con la que se estuvieran moviendo. Esta idea tan simple tiene algunas consecuencias extraordinarias.  
Quizás las más conocidas sean la equivalencia entre masa y energía, resumida en la  famosa ecuación de Einstein E=mc2 (en donde E es la energía, m, la masa y c, la  velocidad de la luz), y la ley de que ningún objeto puede viajar a una velocidad mayor  que la de la luz. Debido a la equivalencia entre energía y masa, la energía que un  objeto adquiere debido a su movimiento se añadirá a su masa, incrementándola. En  otras palabras, cuanto mayor sea la velocidad de un objeto más difícil será aumentar  su velocidad. Este efecto sólo es realmente significativo para objetos que se  muevan a velocidades cercanas a la de la luz. Por ejemplo, a una velocidad de un 10  por 100 de la de la luz la masa de un objeto es sólo un 0,5 por 100 mayor de la  normal, mientras que a un 90 por 100 de la velocidad de la luz la masa sería de más  del doble de la normal. Cuando la velocidad de un objeto se aproxima a la velocidad  de la luz, su masa aumenta cada vez más rápidamente, de forma que cuesta cada  vez más y más energía acelerar el objeto un poco más. De hecho no puede alcanzar  nunca la velocidad de la luz, porque entonces su masa habría llegado a ser infinita, y  por la equivalencia entre masa y energía, habría costado una cantidad infinita de  energía el poner al objeto en ese estado. Por esta razón, cualquier objeto normal  está confinado por la relatividad a moverse siempre a velocidades menores que la  de la luz. Sólo la luz, u otras ondas que no posean masa intrínseca, puede moverse a  la velocidad de la luz. 
Otra consecuencia igualmente notable de la relatividad es el modo en que ha  revolucionado nuestras ideas acerca del espacio y del tiempo. En la teoría de  Newton, si un pulso de luz es enviado de un lugar a otro, observadores diferentes  estarían de acuerdo en el tiempo que duró el viaje (ya que el tiempo es un concepto  absoluto), pero no siempre estarían de acuerdo en la distancia recorrida por la luz  (ya que el espacio no es un concepto absoluto). Dado que la velocidad de la luz es  simplemente la distancia recorrida dividida por el tiempo empleado, observadores  diferentes medirán velocidades de la luz diferentes. En relatividad, por el contrario,  todos los observadores deben estar de acuerdo en lo rápido que viaja la luz. Ellos  continuarán, no obstante, sin estar de acuerdo en la distancia recorrida por la luz, por  lo que ahora ellos también deberán discrepar en el tiempo empleado. (El tiempo  empleado es, después de todo, igual al espacio recorrido, sobre el que los observadores no están de acuerdo, dividido por la velocidad de la luz, sobre la que  los observadores sí están de acuerdo.) En otras palabras, ¡la teoría de la relatividad  acabó con la idea de un tiempo absoluto! Cada observador debe tener su propia  medida del tiempo, que es la que registraría un reloj que se mueve junto a él, y relojes idénticos moviéndose con observadores diferentes no tendrían por qué coincidir. 

Cada observador podría usar un radar para así saber dónde y cuándo ocurrió  cualquier suceso, mediante el envío de un pulso de luz o de ondas de radio. Parte  del pulso se reflejará de vuelta en el suceso y el observador medirá el tiempo que  transcurre hasta recibir el eco. Se dice que el tiempo del suceso es el tiempo medio  entre el instante de emisión del pulso y el de recibimiento del eco. La distancia del  suceso es igual a la mitad del tiempo transcurrido en el viaje completo de ¡da y vuelta, multiplicado por la velocidad de la luz. (Un suceso, en este sentido, es algo  que tiene lugar en un punto específico del espacio y en un determinado instante de  tiempo.) Esta idea se muestra en la figura 2.1, que representa un ejemplo de un  diagrama espacio-tiempo. Usando el procedimiento anterior, observadores en movimiento relativo entre sí asignarán tiempos y posiciones diferentes a un mismo  suceso. Ninguna medida de cualquier observador particular es más correcta que la  de cualquier otro observador, sino que todas son equivalentes y además están relacionadas entre sí. Cualquier observador puede calcular de forma precisa la posición y el tiempo que cualquier otro observador asignará a un determinado  proceso, con tal de que sepa la velocidad relativa del otro observador. 
Hoy en día, se usa este método para medir distancias con precisión, debido a que  podemos medir con más exactitud tiempos que distancias. De hecho, el metro se  define como la distancia recorrida por la luz en 0,000000003335640952 segundos,  medidos por un reloj de cesio. (La razón por la que se elige este número en particular es porque corresponde a la definición histórica del metro, en términos de  dos marcas existentes en una barra de platino concreta que se guarda en París.)  Igualmente, podemos usar una nueva y más conveniente unidad de longitud llamada  segundo-luz. Esta se define simplemente como la distancia que recorre la luz en un  segundo. En la teoría de la relatividad, se definen hoy en día las distancias en  función de tiempos y de la velocidad de la luz, de manera que se desprende que  cualquier observador medirá la misma velocidad de la luz (por definición, 1 metro por  0,000000003335640952 segundos). No hay necesidad de introducir la idea de un  éter, cuya presencia de cualquier manera no puede ser detectada, como mostró el  experimento de Michelson-Morley. La teoría de la relatividad nos fuerza, por el  contrario, a cambiar nuestros conceptos de espacio y tiempo. Debemos aceptar  que el tiempo no está completamente separado e independiente del espacio, sino  que por el contrario se combina con él para formar un objeto llamado espacio tiempo.

Por la experiencia ordinaria sabemos que se puede describir la posición de un punto  en el espacio por tres números o coordenadas. Por ejemplo, uno puede decir que  un punto dentro de una habitación está a tres metros de una pared, a un metro de la  otra y a un metro y medio sobre el suelo. o uno podría especificar que un punto está  a una cierta latitud y longitud y a una cierta altura sobre el nivel del mar. Uno tiene  libertad para usar cualquier conjunto válido de coordenadas, aunque su utilidad  pueda ser muy limitada. Nadie especificaría la posición de la Luna en función de los  kilómetros que diste al norte y al oeste de Piccadilly Circus y del número de metros  que esté sobre el nivel del mar. En vez de eso, uno podría describir la posición de la  Luna en función de su distancia respecto al Sol, respecto al plano que contiene a las  órbitas de los planetas y al ángulo formado entre la línea que une a la Luna y al Sol, y la línea que une al Sol y a alguna estrella cercana, tal como Alfa Centauro. Ni  siquiera estas coordenadas serían de gran utilidad para describir la posición del Sol  en nuestra galaxia, o la de nuestra galaxia en el grupo local de galaxias. De hecho,  se puede describir el universo entero en términos de una colección de pedazos  solapados. En cada pedazo, se puede usar un conjunto diferente de tres coordenadas para especificar la posición de cualquier punto. 
Un suceso es algo que ocurre en un punto particular del espacio y en un instante  específico de tiempo. Por ello, se puede describir por medio de cuatro números o  coordenadas. La elección del sistema de coordenadas es de nuevo arbitraria; uno  puede usar tres coordenadas espaciales cualesquiera bien definidas y una medida  del tiempo. En relatividad, no existe una distinción real entre las coordenadas espaciales y la temporal, exactamente igual a como no hay ninguna diferencia real  entre dos coordenadas espaciales cualesquiera. Se podría elegir un nuevo conjunto  de coordenadas en el que, digamos, la primera coordenada espacial sea una  combinación de la primera y la segunda coordenadas antiguas. Por ejemplo, en vez  de medir la posición de un punto sobre la Tierra en kilómetros al norte de Piccadilly, y  kilómetros al oeste de Piccadilly, se podría usar kilómetros al noreste de Piccadilly y  kilómetros al noroeste de Piccadilly. Similarmente, en relatividad, podría emplearse  una nueva coordenada temporal que fuera igual a la coordenada temporal antigua  (en segundos) más la distancia (en segundos luz) al norte de Piccadilly. 
A menudo resulta útil pensar que las cuatro coordenadas de un suceso especifican  su posición en un espacio cuatridimensional llamado espacio-tiempo. Es imposible  imaginar un espacio cuatridimensional. ¡Personalmente ya encuentro suficientemente difícil visualizar el espacio tridimensional! Sin embargo, resulta fácil  dibujar diagramas de espacios bidimensionales, tales como la superficie de la  Tierra. (La superficie terrestre es bidimensional porque la posición de un punto en  ella puede ser especificada por medio de dos coordenadas, latitud y longitud.)  Generalmente usaré diagramas en los que el tiempo aumenta hacia arriba y una de  las dimensiones espaciales se muestra horizontalmente. Las otras dos dimensiones  espaciales son ignoradas o, algunas veces, una de ellas se indica en perspectiva.  (Estos diagramas, como el que aparece en la figura 2.1, se llaman de espacio 
tiempo.) Por ejemplo, en la figura 2.2 el tiempo se mide hacia arriba en años y la  distancia (proyectada), a lo largo de la línea que va del Sol a Alfa Centauro, se mide  horizontalmente en kilómetros. Los caminos del Sol y de Alfa Centauro, a través del  espacio-tiempo, se representan por las líneas verticales a la izquierda y a la derecha  del diagrama. Un rayo de luz del Sol sigue la línea diagonal y tarda cuatro años en ir  del Sol a Alfa Centauro.

Como hemos visto, las ecuaciones de Maxwell predecían que la velocidad de la luz  debería de ser la misma cualquiera que fuera la velocidad de la fuente, lo que ha  sido confirmado por medidas muy precisas. De ello se desprende que si un pulso  de luz es emitido en un instante concreto, en un punto particular del espacio, entonces, conforme va transcurriendo el tiempo, se irá extendiendo como una esfera  
de luz cuyo tamaño y posición son independientes de la velocidad de la fuente.  Después de una millonésima de segundo la luz se habrá esparcido formando una  esfera con un radio de 300 metros; después de dos millonésimas de segundo el  radio será de 600 metros, y así sucesivamente. Será como las olas que se  extienden sobre la superficie de un estanque cuando se lanza una piedra. Las olas  se extienden como círculos que van aumentando de tamaño conforme pasa el tiempo. Si uno imagina un modelo tridimensional consistente en la superficie 
bidimensional del estanque y la dimensión temporal, las olas circulares que se  expanden marcarán un cono cuyo vértice estará en el lugar y tiempo en que la piedra  golpeó el agua (figura 2.3). De manera similar, la luz, al expandirse desde un suceso dado, forma un cono tridimensional en el espacio-tiempo cuatridimensional. Dicho  cono se conoce como el cono de luz futuro del suceso. De la misma forma, podemos dibujar otro cono, llamado el cono de luz pasado, el cual es el conjunto de  sucesos desde los que un pulso de luz es capaz de alcanzar el suceso dado (figura  2.4). 

Los conos de luz futuro y pasado de un suceso P dividen al espacio-tiempo en tres  regiones (figura 2.5). El futuro absoluto del suceso es la región interior del cono de  luz futuro de P. Es el conjunto de todos los sucesos que pueden en principio ser  afectados por lo que sucede en P. Sucesos fuera del cono de luz de P no pueden ser  alcanzados por señales provenientes de P, porque ninguna de ellas puede viajar  más rápido que la luz. Estos sucesos no pueden, por tanto, ser influidos por lo que  sucede en P. El pasado absoluto de P es la región interna del cono de luz pasado.  Es el conjunto de todos los sucesos desde los que las señales que viajan con  velocidades iguales o menores que la de la luz, pueden alcanzar P. Es, por consiguiente, el conjunto de todos los sucesos que en un principio pueden afectar a  lo que sucede en P. Si se conoce lo que sucede en un instante particular en todos los 

lugares de la región del espacio que cae dentro del cono de luz pasado de P, se  puede predecir lo que sucederá en P. El «resto» es la región del espacio-tiempo que  está fuera de los conos de luz futuro y pasado de P. Sucesos del resto no pueden ni  afectar ni ser afectados por sucesos en P. Por ejemplo, si el Sol cesara de alumbrar  en este mismo instante, ello no afectaría a las cosas de la Tierra en el tiempo  presente porque estaría en la región del resto del suceso correspondiente a 
apagarse el Sol (figura 2.6). Sólo nos enteraríamos ocho minutos después, que es el  tiempo que tarda la luz en alcanzarnos desde el Sol. únicamente entonces estarían  los sucesos de la Tierra en el cono de luz futuro del suceso en el que el Sol se apagó.  De modo similar, no sabemos qué está sucediendo lejos de nosotros en el universo,  en este instante: la luz que vemos de las galaxias distantes partió de ellas hace  millones de años, y en el caso de los objetos más distantes observados, la luz partió  hace unos ocho mil millones de años. Así, cuando miramos al universo, lo vemos tal  como fue en el pasado. 

Si se ignoran los efectos gravitatorios, tal y como Einstein y Poincaré hicieron en  1905, uno tiene lo que se llama la teoría de la relatividad especial. Para cada  suceso en el espacio-tiempo se puede construir un cono de luz (el conjunto de todos  los posibles caminos luminosos en el espacio-tiempo emitidos en ese suceso) y  dado que la velocidad de la luz es la misma para cada suceso y en cada dirección,  todos los conos de luz serán idénticos y estarán orientados en 1 a misma dirección.  La teoría también nos dice que nada puede viajar más rápido que la velocidad de la  luz. Esto significa que el camino de cualquier objeto a través del espacio y del  tiempo debe estar representado por una línea que cae dentro del cono de luz de  cualquier suceso en ella (figura 2.7).

La teoría de la relatividad especial tuvo un gran éxito al explicar por qué la velocidad  de la luz era la misma para todos los observadores (tal y como había mostrado el  experimento de Michelson-Morley) y al describir adecuadamente lo que sucede  cuando los objetos se mueven con velocidades cercanas a la de la luz. Sin embargo,  la teoría era inconsistente con la teoría de la gravitación de Newton, que decía que  los objetos se atraían mutuamente con una fuerza dependiente de la distancia entre  ellos. Esto significaba que si uno movía uno de los objetos, la fuerza sobre el otro  cambiaría instantáneamente. o en otras palabras, los efectos gravitatorios deberían  viajar con velocidad infinita, en vez de con una velocidad igual o menor que la de la  luz, como la teoría de la relatividad especial requería. Einstein realizó entre 1908 y  1914 varios intentos, sin éxito, para encontrar una teoría de la gravedad que fuera  consistente con la relatividad especial. Finalmente, en 1915, propuso lo que hoy en  día se conoce como teoría de la relatividad general. 
Einstein hizo la sugerencia revolucionaria de que la gravedad no es una fuerza como  las otras, sino que es una consecuencia de que el espacio-tiempo no sea plano,como previamente se había supuesto: el espacio-tiempo está curvado, o «deformado», por la distribución de masa y energía en él presente. Los cuerpos  como la Tierra no están forzados a moverse en órbitas curvas por una fuerza llamada  gravedad; en vez de esto, ellos siguen la trayectoria más parecida a una línea recta  en un espacio curvo, es decir, lo que se conoce como una geodésico. Una geodésico es el camino más corto (o más largo) entre dos puntos cercanos. Por  ejemplo, la superficie de la Tierra es un espacio curvo bidimensional. Las geodésicas en la Tierra se llaman círculos máximos, y son el camino más corto entre  dos puntos (figura 2.8). Como la geodésica es el camino más corto entre dos aeropuertos cualesquiera, el navegante de líneas aéreas le dirá al piloto que vuele a  lo largo de ella. En relatividad general, los cuerpos siguen siempre líneas rectas en  el espacio-tiempo cuatridimensional; sin embargo, nos parece que se mueven a lo  largo de trayectorias curvadas en nuestro espacio tridimensional. (Esto es como ver  a un avión volando sobre un terreno montañoso. Aunque sigue una línea recta en el  espacio tridimensional, su sombra seguirá un camino curvo en el suelo bidimensional.) 

La masa del Sol curva el espacio-tiempo de tal modo que, a pesar de que la Tierra  sigue un camino recto en el espacio-tiempo cuatridimensional, nos parece que se  mueve en una órbita circular en el espacio tridimensional. De hecho, las órbitas de  los planetas predichas por la relatividad general son casi exactamente las mismas  que las predichas por la teoría de la gravedad newtoniana. Sin embargo, en el caso de Mercurio, que al ser el planeta más cercano al Sol sufre los efectos gravitatorios  más fuertes y que, además, tiene una órbita bastante alargada, la relatividad general  predice que el eje mayor de su elipse debería rotar alrededor del Sol a un ritmo de  un grado por cada diez mil años. A pesar de lo pequeño de este efecto, ya había  sido observado antes de 1915 y sirvió como una de las primeras confirmaciones de 
la teoría de Einstein. En los últimos años, incluso las desviaciones menores de las  órbitas de los otros planetas respecto de las predicciones newtonianas han sido  medidas por medio del radar, encontrándose que concuerdan con las predicciones  de la relatividad general. 
Los rayos de luz también deben seguir geodésicas en el espacio-tiempo. De nuevo,  el hecho de que el espacio-tiempo sea curvo significa que la luz ya no parece viajar  en líneas rectas en el espacio. Así, la relatividad general predice que la luz debería  ser desviada por los campos gravitatorios. Por ejemplo, la teoría predice que los  conos de luz de puntos cercanos al Sol estarán torcidos hacia dentro, debido a la  presencia de la masa del Sol. Esto quiere decir que la luz de una estrella distante,  que pase cerca del Sol, será desviada un pequeño ángulo, con lo cual la estrella  parecerá estar, para un observador en la Tierra, en una posición diferente a aquella  en la que de hecho está (figura 2.9). Desde luego, si la luz de la estrella pasara  siempre cerca del Sol, no seríamos capaces de distinguir si la luz era desviada  sistemáticamente, o si, por el contrario, la estrella estaba realmente en la posición  donde la vemos. Sin embargo, dado que la Tierra gira alrededor del Sol, diferentes  estrellas parecen pasar por detrás del Sol y su luz es desviada. Cambian, así pues,  su posición aparente con respecto a otras estrellas. 
Normalmente es muy difícil apreciar este efecto, porque la luz del Sol hace imposible  observar las estrellas que aparecen en el cielo cercanas a él. Sin embargo, es  posible observarlo durante un eclipse solar, en el que la Luna se interpone entre la luz  del Sol y la Tierra. Las predicciones de Einstein sobre las desviaciones de la luz no  pudieron ser comprobadas inmediatamente, en 1915, a causa de la primera guerra  mundial, y no fue posible hacerlo hasta 1919, en que una expedición británica, 
observando un eclipse desde África oriental, demostró que la luz era verdaderamente desviada por el Sol, justo como la teoría predecía. Esta comprobación de una teoría alemana por científicos británicos fue reconocida como  un gran acto de reconciliación entre los dos países después de la guerra. Resulta  irónico, que un examen posterior de las fotografías tomadas por aquella expedición  mostrara que los errores cometidos eran tan grandes como el efecto que se trataba  de medir. Sus medidas habían sido o un caso de suerte, o un caso de conocimiento  del resultado que se quería obtener, lo que ocurre con relativa frecuencia en la  ciencia. La desviación de la luz ha sido, no obstante, confirmada con precisión por  numerosas observaciones posteriores.

Otra predicción de la relatividad general es que el tiempo debería transcurrir más  lentamente cerca de un cuerpo de gran masa como la Tierra. Ello se debe a que hay  una relación entre la energía de la luz y su frecuencia (es decir, el número de ondas  de luz por segundo): cuanto mayor es la energía, mayor es la frecuencia. Cuando la  luz viaja hacia arriba en el campo gravitatorio terrestre, pierde energía y, por lo tanto,  su frecuencia disminuye. (Esto significa que el período de tiempo entre una cresta de  la onda y la siguiente aumenta.) A alguien situado arriba le parecería que todo lo que  pasara abajo, en la Tierra, transcurriría más lentamente. Esta predicción fue  comprobada en 1962, usándose un par de relojes muy precisos instalados en la  parte superior e inferior de un depósito de agua. Se encontró que el de abajo, que  estaba más cerca de la Tierra, iba más lento, de acuerdo exactamente con la  relatividad general. La diferencia entre relojes a diferentes alturas de la Tierra es,  hoy en día, de considerable importancia práctica debido al uso de sistemas de  navegación muy precisos, basados en señales provenientes de satélites. Si se ignoraran las predicciones de la relatividad general, ¡la posición que uno calcularía  tendría un error de varios kilómetros! 
Las leyes de Newton del movimiento acabaron con la idea de una posición absoluta  en el espacio. La teoría de la relatividad elimina el concepto de un tiempo absoluto.  Consideremos un par de gemelos. Supongamos que uno de ellos se va a vivir a la  cima de una montaña, mientras que el otro permanece al nivel del mar. El primer  gemelo envejecerá más rápidamente que el segundo. Así, si volvieran a 
encontrarse, uno sería más viejo que el otro. En este caso, la diferencia de edad  seria muy pequeña, pero sería mucho mayor si uno de los gemelos se fuera de viaje  en una nave espacial a una velocidad cercana a la de la luz. Cuando volviera, sería  mucho más joven que el que se quedó en la Tierra. Esto se conoce como la  paradoja de los gemelos, pero es sólo una paradoja si uno tiene siempre metida en  la cabeza la idea de un tiempo absoluto. En la teoría de la relatividad no existe un  tiempo absoluto único, sino que cada individuo posee su propia medida personal del  tiempo, medida que depende de dónde está y de cómo se mueve. 
Antes de 1915, se pensaba en el espacio y en el tiempo como si se tratara de un  marco fijo en el que los acontecimientos tenían lugar, pero que no estaba afectado  por lo que en él sucediera. Esto era cierto incluso en la teoría de la relatividad  especial. Los cuerpos se movían, las fuerzas atraían y repelían, pero el tiempo y el  espacio simplemente continuaban, sin ser afectados por nada. Era natural pensar  que el espacio y el tiempo habían existido desde siempre. 
La situación es, sin embargo, totalmente diferente en la teoría de la relatividad  general. En ella, el espacio y el tiempo son cantidades dinámicas: cuando un cuerpo  se mueve, o una fuerza actúa, afecta a la curvatura del espacio y del tiempo, y, en  contrapartida, la estructura del espacio-tiempo afecta al modo en que los cuerpos se  mueven y las fuerzas actúan. El espacio y el tiempo no sólo afectan, sino que  también son afectados por todo aquello que sucede en el universo. De la misma  manera que no se puede hablar acerca de los fenómenos del universo sin las 
nociones de espacio y tiempo, en relatividad general no tiene sentido hablar del  espacio y del tiempo fuera de los límites del universo. 
En las décadas siguientes al descubrimiento de la relatividad general, estos nuevos  conceptos de espacio y tiempo iban a revolucionar nuestra imagen del universo. La  vieja idea de un universo esencialmente inalterable que podría haber existido, y que  podría continuar existiendo por siempre, fue reemplazada por el concepto de un  universo dinámico, en expansión, que parecía haber comenzado hace cierto tiempo  finito, y que podría acabar en un tiempo finito en el futuro. Esa revolución es el objeto  del siguiente capítulo. Y años después de haber tenido lugar, sería también el punto de arranque de mi trabajo en física teórica. Roger Penrose y yo mostramos cómo la  teoría de la relatividad general de Einstein implicaba que el universo debía tener un  principio y, posiblemente, un final.

Capítulo 3 
EL UNIVERSO EN EXPANSIÓN 
Si se mira el cielo en una clara noche sin luna, los objetos más brillantes que uno ve  son los planetas Venus, Marte, Júpiter y Saturno. También se ve un gran número de  estrellas, que son como nuestro Sol, pero situadas a mucha más distancia de  nosotros. Algunas de estas estrellas llamadas fijas cambian, de hecho, muy ligeramente sus posiciones con respecto a las otras estrellas, cuando la Tierra gira  alrededor del Sol: ¡pero no están fijas en absoluto! Esto se debe a que están  relativamente cerca de nosotros. Conforme la Tierra gira alrededor del Sol, las  vemos desde diferentes posiciones frente al fondo de las estrellas más distantes.  Se trata de un hecho afortunado, pues nos permite medir la distancia entre estas  estrellas y nosotros: cuanto más cerca estén, más parecerán moverse.  
La estrella más cercana, llamada Próxima Centauri, se encuentra a unos cuatro años  luz de nosotros (la luz proveniente de ella tarda unos cuatro años en llegar a la  Tierra), o a unos treinta y siete billones de kilómetros. La mayor parte del resto de  las estrellas observables a simple vista se encuentran a unos pocos cientos de años  luz de nosotros. Para captar la magnitud de estas distancias, digamos que ¡nuestro  Sol está a sólo ocho minutos-luz de distancia! Las estrellas se nos aparecen  esparcidas por todo el cielo nocturno, aunque aparecen particularmente 
concentradas en una banda, que llamamos la Vía Láctea. Ya en 1750, algunos  astrónomos empezaron a sugerir que la aparición de la Vía Láctea podría ser explicada por el hecho de que la mayor parte de las estrellas visibles estuvieran en  una única configuración con forma de disco, un ejemplo de lo que hoy en día  llamamos una galaxia espiral. Sólo unas décadas después, el astrónomo sir William  Herschel confirmó esta idea a través de una ardua catalogación de las posiciones y  las distancias de un gran número de estrellas. A pesar de ello, la idea sólo llegó a  ganar una aceptación completa a principios de nuestro siglo. 
La imagen moderna del universo se remonta tan sólo a 1924, cuando el astrónomo  norteamericano Edwin Hubble demostró que nuestra galaxia no era la única. Había  de hecho muchas otras, con amplias regiones de espacio vacío entre ellas. Para  poder probar esto, necesitaba determinar las distancias que había hasta esas 
galaxias, tan lejanas que, al contrario de lo que ocurre con las estrellas cercanas,  parecían estar verdaderamente fijas. Hubble se vio forzado, por lo tanto, a usar  métodos indirectos para medir esas distancias. Resulta que el brillo aparente de  una estrella depende de dos factores: la cantidad de luz que irradia (su luminosidad)  y lo lejos que está de nosotros. Para las estrellas cercanas, podemos medir sus brillos aparentes y sus distancias, de tal forma que podemos calcular sus luminosidades. Inversamente, si conociéramos la luminosidad de las estrellas de  otras galaxias, podríamos calcular sus distancias midiendo sus brillos aparentes.  Hubble advirtió que ciertos tipos de estrellas, cuando están lo suficientemente cerca  
de nosotros como para que se pueda medir su luminosidad, tienen siempre la  misma luminosidad. Por consiguiente, él argumentó que si encontráramos tales  tipos de estrellas en otra galaxia, podríamos suponer que tendrían la misma luminosidad y calcular, de esta manera, la distancia a esa galaxia. Si pudiéramos  hacer esto para diversas estrellas en la misma galaxia, y nuestros cálculos produjeran siempre el mismo resultado, podríamos estar bastante seguros de nuestra estimación. 

Edwin Hubble calculó las distancias a nueve galaxias diferentes por medio del método anterior. En la actualidad sabemos que nuestra galaxia es sólo una de entre  los varios cientos de miles de millones de galaxias que pueden verse con los modernos telescopios, y que cada una de ellas contiene cientos de miles de millones  de estrellas. La figura 3.1 muestra una fotografía de una galaxia espiral. Creemos  que esta imagen es similar a la de nuestra galaxia si fuera vista por alguien que  viviera en otra galaxia. Vivimos en una galaxia que tiene un diámetro aproximado de  cien mil años luz, y que está girando lentamente. Las estrellas en los brazos de la  espiral giran alrededor del centro con un período de varios cientos de millones de  años.

Nuestro Sol no es más que una estrella amarilla ordinaria, de tamaño medio, situada  cerca del centro de uno de los brazos de la espiral. ¡Ciertamente, hemos recorrido un  largo camino desde los tiempos de Aristóteles y Ptolomeo, cuando creíamos que la  Tierra era el centro del universo! 
Las estrellas están tan lejos de la Tierra que nos parecen simples puntos luminosos.  No podemos apreciar ni su tamaño ni su forma. ¿Cómo entonces podemos clasificar  a las estrellas en distintos tipos? De la inmensa mayoría de las estrellas, sólo  podemos medir una propiedad característica: el color de su luz. Newton descubrió  que cuando la luz atraviesa un trozo de vidrio triangular, lo que se conoce como un  prisma, la luz se divide en los diversos colores que la componen (su espectro), al  igual que ocurre con el arco iris. Al enfocar con un telescopio una estrella o galaxia  particular, podemos observar de modo similar el espectro de la luz proveniente de  esa estrella o galaxia. Estrellas diferentes poseen espectros diferentes, pero el brillo  relativo de los distintos colores es siempre exactamente igual al que se esperaría  encontrar en la luz emitida por un objeto en roja incandescencia. (De hecho, la luz  emitida por un objeto opaco incandescente tiene un aspecto característico que sólo  depende de su temperatura, lo que se conoce como espectro térmico. Esto significa  que podemos averiguar la temperatura de una estrella a partir de su espectro  luminoso.) Además, se observa que ciertos colores muy específicos están ausentes  de los espectros de las estrellas, y que estos colores ausentes pueden variar de una  estrella a otra. Dado que sabemos que cada elemento químico absorbe un conjunto  característico de colores muy específicos, se puede determinar exactamente qué  elementos hay en la atmósfera de una estrella comparando los conjuntos de colores  ausentes de cada elemento con el espectro de la estrella. 
Cuando los astrónomos empezaron a estudiar, en los años veinte, los espectros de  las estrellas de otras galaxias, encontraron un hecho tremendamente peculiar: estas  estrellas poseían los mismos conjuntos característicos de colores ausentes que las  estrellas de nuestra propia galaxia, pero desplazados todos ellos en la misma  cantidad relativa hacia el extremo del espectro correspondiente al color rojo. Para  entender las aplicaciones de este descubrimiento, debemos conocer primero el efecto Doppler. Como hemos visto, la luz visible consiste en fluctuaciones u ondas  del campo electromagnético. La frecuencia (o número de ondas por segundo) de la  luz es extremadamente alta, barriendo desde cuatrocientos hasta setecientos 
millones de ondas por segundo. Las diferentes frecuencias de la luz son lo que el ojo  humano ve como diferentes colores, correspondiendo las frecuencias más bajas al  extremo rojo del espectro y las más altas, al extremo azul. Imaginemos entonces una  fuente luminosa, tal como una estrella, a una distancia fija de nosotros, que emite  ondas de luz con una frecuencia constante. Obviamente la frecuencia de las ondas  que recibimos será la misma que la frecuencia con la que son emitidas (el campo gravitatorio de la galaxia no será lo suficientemente grande como para tener un  efecto significativo). Supongamos ahora que la fuente empieza a moverse hacia  nosotros. Cada vez que la fuente emita la siguiente cresta de onda, estará más  cerca de nosotros, por lo que el tiempo que cada nueva cresta tarde en alcanzarnos  será menor que cuando la estrella estaba estacionaria. Esto significa que el tiempo  entre cada dos crestas que llegan a nosotros es más corto que antes y, por lo tanto,  que el número de ondas que recibimos por segundo (es decir, la frecuencia) es  mayor que cuando la estrella estaba estacionaria. Igualmente, si la fuente se aleja de  nosotros, la frecuencia de las ondas que recibimos será menor que en el supuesto  estacionario. Así pues, en el caso de la luz, esto significa que las estrellas que se  estén alejando de nosotros tendrán sus espectros desplazados hacia el extremo rojo  del espectro (corrimiento hacia el rojo) y las estrellas que se estén acercando  tendrán espectros con un corrimiento hacia el azul. Esta relación entre frecuencia y  velocidad, que se conoce como efecto Doppler, es una experiencia diaria. Si  escuchamos un coche al pasar por la carretera notamos que, cuando se nos aproxima, su motor suena con un tono más agudo de lo normal (lo que corresponde a  
una frecuencia más alta de las ondas sonoras), mientras que cuando se aleja  produce un sonido más grave. El comportamiento de la luz o de las ondas de radio  es similar. De hecho, la policía hace uso del efecto Doppler para medir la velocidad  de los coches a partir de la frecuencia de los pulsos de ondas de radio reflejados por  los vehículos. 
En los años que siguieron al descubrimiento de la existencia de otras galaxias,  Hubble dedicó su tiempo a catalogar las distancias y a observar los espectros de las  galaxias. En aquella época, la mayor parte de la gente pensaba que las galaxias se  moverían de forma bastante aleatorio, por lo que se esperaba encontrar tantos  
espectros con corrimiento hacia el azul como hacia el rojo. Fue una sorpresa  absoluta, por lo tanto, encontrar que la mayoría de las galaxias presentaban un  corrimiento hacia el rojo: ¡casi todas se estaban alejando de nosotros! Incluso más  sorprendente aún fue el hallazgo que Hubble publicó en 1929: ni siquiera el corrimiento de las galaxias hacia el rojo es aleatorio, sino que es directamente  proporcional a la distancia que nos separa de ellas. o, dicho con otras palabras,  ¡cuanto más lejos está una galaxia, a mayor velocidad se aleja de nosotros! Esto  significa que el universo no puede ser estático, como todo el mundo había creído  antes, sino que de hecho se está expandiendo. La distancia entre las diferentes  galaxias está aumentando continuamente. 
El descubrimiento de que el universo se está expandiendo ha sido una de las grandes revoluciones intelectuales del siglo xx. Visto a posteriori, es natural asombrarse de que a nadie se le hubiera ocurrido esto antes. Newton, y algún otro  científico, debería haberse dado cuenta que un universo estático empezaríaenseguida a contraerse bajo la influencia de la gravedad. Pero supongamos que,  por el contrario, el universo se expande. Si se estuviera expandiendo muy lentamente, la fuerza de la gravedad frenaría finalmente la expansión y aquél comenzaría entonces a contraerse. Sin embargo, si se expandiera más deprisa que  
a un cierto valor crítico, la gravedad no sería nunca lo suficientemente intensa como  para detener la expansión, y el universo continuaría expandiéndose por siempre. La  situación sería parecida a lo que sucede cuando se lanza un cohete hacia el espacio  desde la superficie de la Tierra. Si éste tiene una velocidad relativamente baja, la  gravedad acabará deteniendo el cohete, que entonces caerá de nuevo a la Tierra.  Por el contrario, si el cohete posee una velocidad mayor que una cierta velocidad  crítica (de unos once kilómetros por segundo) la gravedad no será lo suficientemente  intensa como para hacerlo regresar de tal forma que se mantendrá alejándose de la  Tierra para siempre. Este comportamiento del universo podría haber sido predicho a  partir de la teoría de la gravedad de Newton, en el siglo xix, en el xviii, o incluso a  finales del xvii. La creencia en un universo estático era tan fuerte que persistió hasta  principios del siglo xx. Incluso Einstein, cuando en 1915 formuló la teoría de la  relatividad general, estaba tan seguro de que el universo tenía que ser estático que  modificó la teoría para hacer que ello fuera posible, introduciendo en sus ecuaciones  la llamada constante cosmológica. Einstein introdujo una nueva fuerza «antigravitatoria», que, al contrario que las otras fuerzas, no provenía de ninguna  fuente en particular, sino que estaba inserta en la estructura misma del espacio 
tiempo. Él sostenía que el espacio-tiempo tenía una tendencia intrínseca a expandirse, - y que ésta tendría un valor que equilibraría exactamente la atracción de  toda la materia en el universo, de modo que sería posible la existencia de un  universo estático. Sólo un hombre estaba dispuesto, según parece, a aceptar la  relatividad general al pie de la letra. Así, mientras Einstein y otros físicos buscaban  modos de evitar las predicciones de la relatividad general de un universo no estático,  el físico y matemático ruso Alexander Friedmann se dispuso, por el contrario, a  explicarlas. 
Friedmann hizo dos suposiciones muy simples sobre el universo: que el universo  parece el mismo desde cualquier dirección desde la que se le observe, y que ello  también sería cierto si se le observara desde cualquier otro lugar. A partir de estas  dos ideas únicamente, Friedmann demostró que no se debería esperar que el universo fuera estático. De hecho, en 1922, varios años antes del descubrimiento de  
Edwin Hubble, ¡Friedmann predijo exactamente lo que Hubble encontró! 
La suposición de que el universo parece el mismo en todas direcciones, no es cierta  en la realidad. Por ejemplo, como hemos visto, las otras estrellas de nuestra galaxia  forman una inconfundible banda de luz a lo largo del cielo, llamada Vía Láctea. Pero  si nos concentramos en las galaxias lejanas, parece haber más o menos el mismo número de ellas en cada dirección. Así, el universo parece ser aproximadamente el  mismo en cualquier dirección, con tal de que se le analice a gran escala, comparada  con la distancia entre galaxias, y se ignoren las diferencias a pequeña escala.  Durante mucho tiempo, esto fue justificación suficiente para la suposición de 
Friedmann, tomada como una aproximación grosera del mundo real. Pero recientemente, un afortunado accidente reveló que la suposición de Friedmann es de  hecho una descripción extraordinariamente exacta de nuestro universo.  
En 1965, dos físicos norteamericanos de los laboratorios de la Bell Telephone en  Nueva Jersey, Arno Penzias y Robert Wilson, estaban probando un detector de  microondas extremadamente sensible, (Las microondas son iguales a las ondas  luminosas, pero con una frecuencia del orden de sólo diez mil millones de ondas por  segundo.) Penzias y Wilson se sorprendieron al encontrar que su detector captaba  más ruido del que esperaban. El ruido no parecía provenir de ninguna dirección en  particular. Al principio descubrieron excrementos de pájaro en su detector, por lo  que comprobaron todos los posibles defectos de funcionamiento, pero pronto los  desecharon. Ellos sabían que cualquier ruido proveniente de dentro de la atmósfera  sería menos intenso cuando el detector estuviera dirigido hacia arriba que cuando no  lo estuviera, ya que los rayos luminosos atraerían mucha más atmósfera cuando se  recibieran desde cerca del horizonte que cuando se recibieran directamente desde  arriba. El ruido extra era el mismo para cualquier dirección desde la que se  observara, de forma que debía provenir de fuera de la atmósfera. El ruido era  también el mismo durante el día, y durante la noche, y a lo largo de todo el año, a  pesar de que la Tierra girara sobre su eje y alrededor del Sol. Esto demostró que la  radiación debía provenir de más allá del sistema solar, e incluso desde más allá de  nuestra galaxia, pues de lo contrario variaría cuando el movimiento de la Tierra  hiciera que el detector apuntara en diferentes direcciones. De hecho, sabemos que  la radiación debe haber viajado hasta nosotros a través de la mayor parte del  universo observable, y dado que parece ser la misma en todas las direcciones, el  universo debe también ser el mismo en todas las direcciones, por lo menos a gran  escala. En la actualidad, sabemos que en cualquier dirección que miremos, el ruido  nunca varía más de una parte en diez mil. Así, Penzias y Wilson tropezaron inconscientemente con una confirmación extraordinariamente precisa de la primera  suposición de Friedmann. 
Aproximadamente al mismo tiempo, dos físicos norteamericanos de la cercana  Universidad de Princeton, Bob Dicke y Jim Peebles, también estaban interesados  en las microondas. Estudiaban una sugerencia hecha por George Gamow (que  había sido alumno de Alexander Friedmann) relativa a que el universo en sus primeros instantes debería haber sido muy caliente y denso, para acabar blanco  incandescente. Dicke y Peebles argumentaron que aún deberíamos ser capaces de 

ver el resplandor de los inicios del universo, porque la luz proveniente de lugares muy  distantes estaría alcanzándonos ahora. Sin embargo, la expansión del universo  implicaría que esta luz debería estar tan tremendamente desplazada hacia el rojo  que nos llegaría hoy en día como radiación de microondas. Cuando Dicke y Peebles estaban estudiando cómo buscar esta radiación, Penzias y Wilson se  enteraron del objetivo de ese trabajo y comprendieron que ellos ya habían encontrado dicha radiación. Gracias a este trabajo, Penzias y Wilson fueron galardonados con el premio Nobel en 1978 (¡lo que parece ser bastante injusto con  Dicke y Peebles, por no mencionar a Gamow!). 
A primera vista, podría parecer que toda esta evidencia de que el universo parece el  mismo en cualquier dirección desde la que miremos sugeriría que hay algo especial  en cuanto a nuestra posición en el universo. En particular, podría pensarse que, si  observamos a todas las otras galaxias alejarse de nosotros, es porque estamos en  el centro del universo. Hay, sin embargo, una explicación alternativa: el universo  podría ser también igual en todas las direcciones si lo observáramos desde cualquier otra galaxia. Esto, como hemos visto, fue la segunda suposición de  Friedmann. No se tiene evidencia científica a favor o en contra de esta suposición.  Creemos en ella sólo por razones de modestia: ¡sería extraordinariamente curioso  que el universo pareciera idéntico en todas las direcciones a nuestro alrededor, y  que no fuera así para otros puntos del universo! En el modelo de Friedmann, todas  las galaxias se están alejando entre sí unas de otras. La situación es similar a un  globo con cierto número de puntos dibujados en él, y que se va hinchando uniformemente. Conforme el globo se hincha, la distancia entre cada dos puntos  aumenta, a pesar de lo cual no se puede decir que exista un punto que sea el centro  de la expansión. Además, cuanto más lejos estén los puntos, se separarán con  mayor velocidad. Similarmente, en el modelo de Friedmann la velocidad con la que  dos galaxias cualesquiera se separan es proporcional a la distancia entre ellas. De  esta forma, predecía que el corrimiento hacia el rojo de una galaxia debería ser  directamente proporcional a su distancia a nosotros, exactamente lo que Hubble 
encontró. A pesar del éxito de su modelo y de sus predicciones de las observaciones de Hubble, el trabajo de Friedmann siguió siendo desconocido en el  mundo occidental hasta que en 1935 el físico norteamericano Howard Robertson y el  matemático británico Arthur Walker crearon modelos similares en respuesta al descubrimiento por Hubble de la expansión uniforme del universo. Aunque Friedmann encontró sólo uno, existen en realidad tres tipos de modelos que obedecen a las dos suposiciones fundamentales de Friedmann. En el primer tipo (el  que encontró Friedmann), el universo se expande lo suficientemente lento como para  que la atracción gravitatoria entre las diferentes galaxias sea capaz de frenar y finalmente detener la expansión. Las galaxias entonces se empiezan a acercar las  unas a las otras y el universo se contrae. La figura 3.2 muestra cómo cambia, conforme aumenta el tiempo, la distancia entre dos galaxias vecinas. Ésta empieza  siendo igual a cero, aumenta hasta llegar a un máximo y luego disminuye hasta  hacerse cero de nuevo. En el segundo tipo de solución, el universo se expande tan  rápidamente que la atracción gravitatoria no puede pararlo, aunque sí que lo frena un  poco. La figura 3.3 muestra la separación entre dos galaxias vecinas en este  modelo. Empieza en cero y con el tiempo sigue aumentando, pues las galaxias  continúan separándose con una velocidad estacionaria. Por último, existe un tercer  tipo de solución, en el que el universo se está expandiendo sólo con la velocidad  justa para evitar colapsarse. La separación en este caso, mostrada en la figura 3.4,  también empieza en cero y continúa aumentando siempre. Sin embargo, la velocidad con la que las galaxias se están separando se hace cada vez más pequeña, aunque nunca llega a ser nula. 


Figure 3:4 
Una característica notable del primer tipo de modelo de Friedmann es que, en él, el  universo no es infinito en el espacio, aunque tampoco tiene ningún límite. La  gravedad es tan fuerte que el espacio se curva cerrándose sobre sí mismo, resultando parecido a la superficie de la Tierra. Si uno se mantiene viajando sobre la  
superficie de la Tierra en una cierta dirección, nunca llega frente a una barrera  infranqueable o se cae por un precipicio, sino que finalmente regresa al lugar de  donde partió. En el primer modelo de Friedmann, el espacio es justo como esto,  pero con tres dimensiones en vez de con dos, como ocurre con la superficie  terrestre. La cuarta dimensión, el tiempo, también tiene una extensión finita, pero es  como una línea con dos extremos o fronteras, un principio y un final. Se verá más  adelante que cuando se combina la relatividad general con el principio de incertidumbre de la mecánica cuántica, es posible que ambos, espacio y tiempo,  sean finitos, sin ningún tipo de borde o frontera. 
La idea de que se podría ir en línea recta alrededor del universo y acabar donde se  empezó es buena para la ciencia-ficción, pero no tiene demasiada relevancia  práctica, pues puede verse que el universo se colapsaría de nuevo a tamaño cero  antes de que se pudiera completar una vuelta entera. Uno tendría que viajar más  rápido que la luz, lo que es imposible, para poder regresar al punto de partida antes  de que el universo tuviera un final. 
En el primer tipo de modelo de Friedmann, el que se expande primero y luego se  colapsa, el espacio está curvado sobre sí mismo, al igual que la superficie de la  Tierra. Es, por lo tanto, finito en extensión. En el segundo tipo de modelo, el que se  expande por siempre, el espacio está curvado al contrario, es decir, como la 
superficie de una silla de montar. Así, en este caso el espacio es infinito.  Finalmente, en el tercer tipo, el que posee la velocidad crítica de expansión, el  espacio no está curvado (y, por lo tanto, también es infinito).

Pero, ¿cuál de los modelos de Friedmann describe a nuestro universo? ¿Cesará  alguna vez el universo su expansión y empezará a contraerse, o se expandirá por  siempre? Para responder a estas cuestiones, necesitamos conocer el ritmo actual  de expansión y la densidad media presente. Si la densidad es menor que un cierto  valor crítico, determinado por el ritmo de expansión, la atracción gravitatoria será  demasiado débil para poder detener la expansión. Si la densidad es mayor que el  valor crítico, la gravedad parará la expansión en algún tiempo futuro y hará que el  universo vuelva a colapsarse. 
Podemos determinar el ritmo actual de expansión, midiendo a través del efecto  Doppler las velocidades a las que las otras galaxias se alejan de nosotros. Esto  puede hacerse con mucha precisión. Sin embargo, las distancias a las otras galaxias no se conocen bien porque sólo podemos medirlas indirectamente. Así,  todo lo que sabemos es que el universo se expande entre un cinco y un diez por 100  
cada mil millones de años. Sin embargo, nuestra incertidumbre con respecto a la  densidad media actual del universo es incluso mayor. Si sumamos las masas de  todas las estrellas, que podemos ver tanto en nuestra galaxia como en las otras  galaxias, el total es menos de la centésima parte de la cantidad necesaria para  detener la expansión del universo, incluso considerando la estimación más baja del  ritmo de expansión. Nuestra galaxia y las otras galaxias deben contener, no obstante, una gran cantidad de «materia oscura» que no se puede ver directamente,  pero que sabemos que debe existir, debido a la influencia de su atracción gravitatoria sobre las órbitas de las estrellas en las galaxias. Además, la mayoría de  
las galaxias se encuentran agrupadas en racimos, y podemos inferir igualmente la  presencia de aún más materia oscura en los espacios intergalácticos de los racimos, debido a su efecto sobre el movimiento de las galaxias. Cuando sumamos  toda esta materia oscura, obtenemos tan sólo la décima parte, aproximadamente, de  la cantidad requerida para detener la expansión. No obstante, no podemos excluir la  posibilidad de que pudiera existir alguna otra forma de materia, distribuida casi  uniformemente a lo largo y ancho del universo, que aún no hayamos detectado y que  podría elevar la densidad media del universo por encima del valor crítico necesario  para detener la expansión. La evidencia presente sugiere, por lo tanto, que el universo se expandirá probablemente por siempre, pero que de lo único que podemos estar verdaderamente seguros es de que si el universo se fuera a colapsar, no lo haría como mínimo en otros diez mil millones de años, ya que se ha  estado expandiendo por lo menos esa cantidad de tiempo. Esto no nos debería  preocupar indebidamente: para entonces, al menos que hayamos colonizado más  allá del sistema solar, ¡la humanidad hará tiempo que habrá desaparecido, extinguida junto con nuestro Sol!

Todas las soluciones de Friedmann comparten el hecho de que en algún tiempo  pasado (entre diez y veinte mil millones de años) la distancia entre galaxias vecinas  debe haber sido cero. En aquel instante, que llamamos big bang, la densidad del  universo y la curvatura del espacio-tiempo habrían sido infinitas. Dado que las  matemáticas no pueden manejar realmente números infinitos, esto significa que la  teoría de la relatividad general (en la que se basan las soluciones de Friedmann)  predice que hay un punto en el universo en donde la teoría en sí colapsa. Tal punto es  un ejemplo de lo que los matemáticos llaman una singularidad. En realidad, todas  nuestras teorías científicas están formuladas bajo la suposición de que el espacio tiempo es uniforme y casi plano, de manera que ellas dejan de ser aplicables en la  singularidad del big bang, en donde la curvatura del espacio-tiempo es infinita. Ello  significa que aunque hubiera acontecimientos anteriores al big bang, no se podrían  utilizar para determinar lo que sucedería después, ya que toda capacidad de predicción fallaría en el big bang. Igualmente, si, como es el caso, sólo sabemos lo  que ha sucedido después del big bang, no podremos determinar lo que sucedió  antes. Desde nuestro punto de vista, los sucesos anteriores al big bang no pueden  tener consecuencias, por lo que no deberían formar parte de los modelos científicos  del universo. Así pues, deberíamos extraerlos de cualquier modelo y decir que el  tiempo tiene su principio en el big bang. 
A mucha gente no le gusta la idea de que el tiempo tenga un principio, probablemente porque suena a intervención divina. (La Iglesia católica, por el contrario, se apropió del modelo del big bang y en 1951 proclamó oficialmente que  estaba de acuerdo con la Biblia.) Por ello, hubo un buen número de intentos para  evitar la conclusión de que había habido un big bang. La propuesta que consiguió un  apoyo más amplio fue la llamada teoría del estado estacionario (steady state). Fue  sugerida, en 1948, por dos refugiados de la Austria ocupada por los nazis, Hermann  Bond y Thomas Gold, junto con un británico, Fred Hoyle, que había trabajado con  ellos durante la guerra en el desarrollo del radar. La idea era que conforme las  galaxias se iban alejando unas de otras, nuevas galaxias se formaban continuamente  en las regiones intergalácticas, a partir de materia nueva que era creada de forma  continua. El universo parecería, así pues, aproximadamente el mismo en todo tiempo  y en todo punto del espacio. La teoría del estado estacionario requería una modificación de la relatividad general para permitir la creación continua de materia,  pero el ritmo de creación involucrado era tan bajo (aproximadamente una partícula por kilómetro cúbico al año) que no estaba en conflicto con los experimentos. La  teoría era una buena teoría científica, en el sentido descrito en el capítulo 1: era  simple y realizaba predicciones concretas que podrían ser comprobadas por la  observación. Una de estas predicciones era que el número de galaxias, u objetos  similares en cualquier volumen dado del espacio, debería ser el mismo en donde  quiera y cuando quiera que miráramos en el universo. Al final de los años cincuenta 

y principio de los sesenta, un grupo de astrónomos dirigido por Martin Ryle (quien  también había trabajado con Bond, Gold y Hoyle en el radar durante la guerra)  realizó, en Cambridge, un estudio sobre fuentes de ondas de radio en el espacio  exterior. El grupo de Cambridge demostró que la mayoría de estas fuentes de radio  
deben residir fuera de nuestra galaxia (muchas de ellas podían ser identificadas  verdaderamente con otras galaxias), y, también, que había muchas más fuentes  débiles que intensas. Interpretaron que las fuentes débiles eran las más distantes,  mientras que las intensas eran las más cercanas. Entonces resultaba haber menos  fuentes comunes por unidad de volumen para las fuentes cercanas que para las  lejanas. Esto podría significar que estamos en una región del universo en la que las  fuentes son más escasas que en el resto. Alternativamente, podría significar que las  fuentes eran más numerosas en el pasado, en la época en que las ondas de radio  comenzaron su viaje hacia nosotros, que ahora. Cualquier explicación contradecía  las predicciones de la teoría del estado estacionario. Además, el descubrimiento de  la radiación de microondas por Penzias y Wilson en 1965 también indicó que el  universo debe haber sido mucho más denso en el pasado. La teoría del estado  estacionario tenía, por lo tanto, que ser abandonada. 
Otro intento de evitar la conclusión de que debe haber habido un big bang y, por lo  tanto, un principio del tiempo, fue realizado por dos científicos rusos, Evgenii Lifshitz  e Isaac Khalatnikov, en 1963. Ellos sugirieron que el big bang podría ser, únicamente, una peculiaridad de los modelos de Friedmann, que después de todo  no eran más que aproximaciones al universo real. Quizás, de todos los modelos que  eran aproximadamente como el universo real, sólo los de Friedmann contuvieran una  singularidad como la del big bang. En los modelos de Friedmann, todas las galaxias  se están alejando directamente unas de otras, de tal modo que no es sorprendente  que en algún tiempo pasado estuvieran todas juntas en el mismo lugar. En el  
universo real, sin embargo, las galaxias no tienen sólo un movimiento de separación  de unas con respecto a otras, sino que también tienen pequeñas velocidades laterales. Así, en realidad, nunca tienen por qué haber estado todas en el mismo  lugar exactamente, sino simplemente muy cerca unas de otras. Quizás entonces el  universo en expansión actual no habría resultado de una singularidad como el big  bang, sino de, una fase previa en contracción. Cuando el universo se colapsó, las  partículas que lo formaran podrían no haber colisionado todas entre sí, sino que se  habrían entrecruzado y separado después, produciendo la expansión actual del 
universo. ¿Cómo podríamos entonces distinguir si el universo real ha comenzado con  un big bang o no? Lo que Lifshitz y Khalatnikov hicieron fue estudiar modelos del  universo que eran aproximadamente como los de Friedmann, pero que tenían en  cuenta las irregularidades y las velocidades aleatorias de las galaxias en el universo  real. Demostraron que tales 'Modelos podrían comenzar con un big bang, incluso a  pesar de que las galaxias ya no estuvieran separándose directamente unas de otras, pero sostuvieron que ello sólo seguía siendo posible en ciertos modelos excepcionales en los que las galaxias se movían justamente en la forma adecuada.  Argumentaron que, ya que parece haber infinitamente más modelos del tipo Friedmann sin una singularidad como la del big bang que con una, se debería  concluir que en realidad no ha existido el big bang. Sin embargo, más tarde se  dieron cuenta de que había una clase mucho más general de modelos del tipo  Friedmann que sí contenían singularidades, y en los que las galaxias no tenían que  estar moviéndose de un modo especial. Así pues, retiraron su afirmación en 1970. 

El trabajo de Lifshitz y Khalatnikov fue muy valioso porque demostró que el universo  podría haber tenido una singularidad, un big bang, si la teoría de la relatividad  general era correcta. Sin embargo, no resolvió la cuestión fundamental: ¿predice la  teoría de la relatividad general que nuestro universo debería haber tenido un big  bang, un principio del tiempo? La respuesta llegó a través de una aproximación  completamente diferente, comenzada por un físico y matemático británico, Roger  Penrose, en 1965. Usando el modo en que los conos de luz se comportan en la  relatividad general, junto con el hecho de que la gravedad es siempre atractiva,  demostró que una estrella que se colapsa bajo su propia gravedad está atrapada en  una región cuya superficie se reduce con el tiempo a tamaño cero. Y, si la superficie  de la región se reduce a cero, lo mismo debe ocurrir con su volumen. Toda la  materia de la estrella estará comprimida en una región de volumen nulo, de tal forma  que la densidad de materia y la curvatura del espacio-tiempo se harán infinitas. En  otras palabras, se obtiene una singularidad contenida dentro de una región del espacio-tiempo llamada agujero negro. 
A primera vista, el resultado de Penrose sólo se aplica a estrellas. No tiene nada  que ver con la cuestión de si el universo entero tuvo, en el pasado, una singularidad  del tipo del big bang. No obstante, cuando Penrose presentó su teorema, yo era un  estudiante de investigación que buscaba desesperadamente un problema con el que  completar la tesis doctoral. Dos años antes, se me había diagnosticado la 
enfermedad ALS, comúnmente conocida como enfermedad de Lou Gehrig o de las  neuronas motoras, y se me había dado a entender que sólo me quedaban uno o dos  años de vida. En estas circunstancias no parecía tener demasiado sentido trabajar  en la tesis doctoral, pues no esperaba sobrevivir tanto tiempo. A pesar de eso,  habían transcurrido dos años y no me encontraba mucho peor. De hecho, las cosas  me iban bastante bien y me había prometido con una chica encantadora, Jane Wilde.  Pero para poderme casar, necesitaba un trabajo, y para poderlo obtener, necesitaba  el doctorado. 

En 1965, leí acerca del teorema de Penrose según el cual cualquier cuerpo que  sufriera un colapso gravitatorio debería finalmente formar una singularidad. Pronto comprendí que si se invirtiera la dirección del tiempo en el teorema de Penrose, de  forma que el colapso se convirtiera en una expansión, las condiciones del teorema  seguirían verificándose, con tal de que el universo a gran escala fuera, en la  actualidad, aproximadamente como un modelo de Friedmann. El teorema de 
Penrose había demostrado que cualquier estrella que se colapse debe acabar en  una singularidad. El mismo argumento con el tiempo invertido demostró que cualquier universo en expansión, del tipo de Friedmann, debe haber comenzado en  una singularidad. Por razones técnicas, el teorema de Penrose requería que el  universo fuera infinito espacialmente. Consecuentemente, sólo podía utilizarlo para  probar que debería haber una singularidad si el universo se estuviera expandiendo lo  
suficientemente rápido como para evitar colapsarse de nuevo (ya que sólo estos  modelos de Friedmann eran infinitos espacialmente)- 

Durante los años siguientes, me dediqué a desarrollar nuevas técnicas matemáticas  para eliminar el anterior y otros diferentes requisitos técnicos de los teoremas, que  probaban que las singularidades deben existir. El resultado final fue un artículo  conjunto entre Penrose y yo, en 1970, que al final probó que debe haber habido una  singularidad como la del big bang, con la única condición de que la relatividad  general sea correcta y que el universo contenga tanta materia como observamos.  
Hubo una fuerte oposición a nuestro trabajo, por parte de los rusos, debido a su  creencia marxista en el determinismo científico, y por parte de la gente que creía que  la idea en sí de las singularidades era repugnante y estropeaba la belleza de la  teoría de Einstein. No obstante, uno no puede discutir en contra de un teorema  matemático. Así, al final, nuestro trabajo llegó a ser generalmente aceptado y, hoy en  día, casi todo el mundo supone que el universo comenzó con una singularidad como  la del big bang. Resulta por eso irónico que, al haber cambiado mis ideas, esté  tratando ahora de convencer a los otros físicos de que no hubo en realidad 
singularidad al principio del universo. Como veremos más adelante, ésta puede  desaparecer una vez que los efectos cuánticos se tienen en cuenta. 
Hemos visto en este capítulo cómo, en menos de medio siglo, nuestra visión del  universo, formada durante milenios, se ha transformado. El descubrimiento de  Hubble de que el universo se está expandiendo, y el darnos cuenta de la insignificancia de nuestro planeta en la inmensidad del universo, fueron sólo el punto  de partida. Conforme la evidencia experimental y teórica se iba acumulando, se  clarificaba cada vez más que el universo debe haber tenido un principio en el tiempo,  hasta que en 1970 esto fue finalmente probado por Penrose y por mí, sobre la base  de la teoría de la relatividad general de Einstein. Esa prueba demostró que la  relatividad general es sólo una teoría incompleta: no puede decirnos cómo empezó  el universo, porque predice que todas las teorías físicas, incluida ella misma, fallan al  principio del universo. No obstante, la relatividad general sólo pretende ser una  teoría parcial, de forma que lo que el teorema de la singularidad realmente muestra es que debió haber habido un tiempo, muy al principio del universo, en que éste era  tan pequeño que ya no se pueden ignorar los efectos de pequeña escala de la otra  gran teoría parcial del siglo xx, la mecánica cuántica. Al principio de los años  setenta, nos vimos forzados a girar nuestra búsqueda de un entendimiento del universo, desde nuestra teoría de lo extraordinariamente inmenso, hasta nuestra  teoría de lo extraordinariamente diminuto. Esta teoría, la mecánica cuántica, se  describirá a continuación, antes de volver a explicar los esfuerzos realizados para  combinar las dos teorías parciales en una única teoría cuántica de la gravedad.

Capítulo 4 
EL PRINCIPIO DE 
INCERTIDUMBRE 
El éxito de las teorías científicas, y en particular el de la teoría de la gravedad de  Newton, llevó al científico francés marqués de Laplace a argumentar, a principios del  siglo xix, que el universo era completamente determinista. Laplace sugirió que debía  existir un conjunto de leyes científicas que nos permitirían predecir todo lo que  sucediera en el universo, con tal de que conociéramos el estado completo del 
universo en un instante de tiempo. Por ejemplo, si supiéramos las posiciones y  velocidades del Sol y de los planetas en un determinado momento, podríamos usar  entonces las leyes de Newton para calcular el estado del sistema solar en cualquier  otro instante. El determinismo parece bastante obvio en este caso, pero Laplace fue  
más lejos hasta suponer que había leyes similares gobernando todos los fenómenos,  incluido el comportamiento humano. 
La doctrina del determinismo científico fue ampliamente criticada por diversos  sectores, que pensaban que infringía la libertad divina de intervenir en el mundo,  pero, a pesar de ello, constituyó el paradigma de la ciencia hasta los primeros años  de nuestro siglo. Una de las primeras indicaciones de que esta creencia habría de  ser abandonada llegó cuando los cálculos de los científicos británicos lord Rayleigh y  sir James Jeans sugirieron que un objeto o cuerpo caliente, tal como una estrella,  debería irradiar energía a un ritmo infinito. De acuerdo con las leyes en las que se  creía en aquel tiempo, un cuerpo caliente tendría que emitir ondas electromagnéticas  (tales como ondas de radio, luz visible o rayos X) con igual intensidad a todas las  frecuencias. Por ejemplo, un cuerpo caliente debería irradiar la misma cantidad de  energía, tanto en ondas con frecuencias comprendidas entre uno y dos billones de  ciclos por segundo, como en ondas con frecuencias comprendidas entre dos y tres  billones de ciclos por segundo. Dado que el número de ciclos por segundo es  ilimitado, esto significaría entonces que la energía total irradiada sería infinita. 
Para evitar este resultado, obviamente ridículo, el científico alemán Max Planck sugirió en 1900 que la luz, los rayos X y otros tipos de ondas no podían ser emitidos  en cantidades arbitrarias, sino sólo en ciertos paquetes que él llamó «cuantos».  Además, cada uno de ellos poseía una cierta cantidad de energía que era tanto  mayor cuanto más alta fuera la frecuencia de las ondas, de tal forma que para  frecuencias suficientemente altas la emisión de un único cuanto requeriría más  energía de la que se podía obtener. Así la radiación de altas frecuencias se  reduciría, y el ritmo con el que el cuerpo perdía energía sería, por lo tanto, finito.

La hipótesis cuántica explicó muy bien la emisión de radiación por cuerpos calientes,  pero sus aplicaciones acerca del determinismo no fueron comprendidas hasta 1926,  cuando otro científico alemán, Werner Heisenberg, formuló su famoso principio de  incertidumbre. Para poder predecir la posición y la velocidad futuras de una  partícula, hay que ser capaz de medir con precisión su posición y velocidad actuales.  El modo obvio de hacerlo es iluminando con luz la partícula. Algunas de las ondas  luminosas serán dispersadas por la partícula, lo que indicará su posición. Sin  embargo, uno no podrá ser capaz de determinar la posición de la partícula con  mayor precisión que la distancia entre dos crestas consecutivas de la onda luminosa,  por lo que se necesita utilizar luz de muy corta longitud de onda para poder medir la 
posición de la partícula con precisión. Pero, según la hipótesis de Planck, no se  puede usar una cantidad arbitrariamente pequeña de luz; se tiene que usar como  mínimo un cuanto de luz. Este cuanto perturbará la partícula, cambiando su velocidad en una cantidad que no puede ser predicha. Además, cuanto con mayor  precisión se mida la posición, menor habrá de ser la longitud de onda de la luz que  se necesite y, por lo tanto, mayor será la energía del cuanto que se haya de usar. Así  la velocidad de la partícula resultará fuertemente perturbada. En otras palabras,  cuanto con mayor precisión se trate de medir la posición de la partícula, con menor  exactitud se podrá medir su velocidad, y viceversa. Heisenberg demostró que la  incertidumbre en la posición de la partícula, multiplicada por la incertidumbre en su  velocidad y por la masa de la partícula, nunca puede ser más pequeña que una  cierta cantidad, que se conoce como constante de Planck. Además, este límite no  depende de la forma en que uno trata de medir la posición o la velocidad de la  partícula, o del tipo de partícula: el principio de incertidumbre de Heisenberg es una  propiedad fundamental, ineludible, del mundo. 
El principio de incertidumbre tiene profundas aplicaciones sobre el modo que tenemos de ver el mundo. Incluso más de cincuenta años después, éstas no han  sido totalmente apreciadas por muchos filósofos, y aún son objeto de mucha controversia. El principio de incertidumbre marcó el final del sueño de Laplace de  
una teoría de la ciencia, un modelo del universo que sería totalmente determinista:  ciertamente, ¡no se pueden predecir los acontecimientos futuros con exactitud si ni  siquiera se puede medir el estado presente del universo de forma precisa! Aún  podríamos suponer que existe un conjunto de leyes que determina completamente  los acontecimientos para algún ser sobrenatural, que podría observar el estado  presente del universo sin perturbarle. Sin embargo, tales modelos del universo no  son de demasiado interés para nosotros, ordinarios mortales. Parece mejor emplear el principio de economía conocido como «cuchilla de Occam» y eliminar  todos los elementos de la teoría que no puedan ser observados. Esta aproximación  llevó en 1920 a Heisenberg, Erwin Schrijdinger y Paul Dirae a reformular la mecánica  con una nueva teoría llamada mecánica cuántica, basada en el principio de incertidumbre. En esta teoría las partículas ya no poseen posiciones y velocidades  definidas por separado, pues éstas no podrían ser observadas. En vez de ello, las  partículas tienen un estado cuántico, que es una combinación de posición y velocidad. 
En general, la mecánica cuántica no predice un único resultado de cada observación. En su lugar, predice un cierto número de resultados posibles y nos da  las probabilidades de cada uno de ellos. Es decir, si se realizara la misma medida  sobre un gran número de sistemas similares, con las mismas condiciones de partida  en cada uno de ellos, se encontraría que el resultado de la medida sería A un cierto  número de veces, B otro número diferente de veces, y así sucesivamente. Se podría  predecir el número aproximado de veces que se obtendría el resultado A o el B, pero  no se podría predecir el resultado especifico de una medida concreta. Así pues, la  mecánica cuántica introduce un elemento inevitable de incapacidad de predicción,  una aleatoriedad en la ciencia. Einstein se opuso fuertemente a ello, a pesar del  
importante papel que él mismo había jugado en el desarrollo de estas ideas.  Einstein recibió el premio Nobel por su contribución a la teoría cuántica. No  obstante, Einstein nunca aceptó que el universo estuviera gobernado por el azar.  Sus ideas al respecto están resumidas en su famosa frase «Dios no juega a los  dados». La mayoría del resto de los científicos, sin embargo, aceptaron sin problemas la mecánica cuántica porque estaba perfectamente de acuerdo con los  experimentos. Verdaderamente, ha sido una teoría con un éxito sobresaliente, y en  ella se basan casi toda la ciencia y la tecnología modernas. Gobierna el comportamiento de los transistores y de los circuitos integrados, que son los componentes esenciales de los aparatos electrónicos, tales como televisores y ordenadores, y también es la base de la química y de la biología modernas. Las  únicas áreas de las ciencias físicas en las que la mecánica cuántica aún no ha sido  
adecuadamente incorporada son las de la gravedad y la estructura a gran escala del  universo. 
Aunque la luz está formada por ondas, la hipótesis de los cuantos de Planck nos dice  que en algunos aspectos se comporta como si estuviera compuesta por partículas:  sólo puede ser emitida o absorbida en paquetes o cuantos. Igualmente, el principio  de incertidumbre de Heisenberg implica que las partículas se comportan en algunos  aspectos como ondas: no tienen una posición bien definida, sino que están 
«esparcidas» con una cierta distribución de probabilidad. La teoría de la mecánica  cuántica está basada en una descripción matemática completamente nueva, que ya  no describe al mundo real en términos de partículas y ondas; sólo las observaciones  del mundo pueden ser descritas en esos términos. Existe así, por tanto, una  dualidad entre ondas y partículas en la mecánica cuántica: para algunos fines es útil pensar en las partículas como ondas, mientras que para otros es mejor pensar en las  ondas como partículas. Una consecuencia importante de lo anterior, es que se  puede observar el fenómeno llamado de interferencia entre dos conjuntos de ondas o  de partículas. Es decir, las crestas de uno de los conjuntos de ondas pueden  coincidir con los valles del otro conjunto. En este caso los dos conjuntos de ondas se  cancelan mutuamente, en vez de sumarse formando una onda más intensa, como se  podría esperar (figura 4.1). Un ejemplo familiar de interferencia en el caso de la luz lo  constituyen los colores que con frecuencia aparecen en las pompas de jabón. Éstos  están causados por la reflexión de la luz en las dos caras de la delgada capa de  agua que forma la pompa. La luz blanca está compuesta por ondas luminosas de  todas las longitudes de ondas o, lo que es lo mismo, de todos los colores. Para  ciertas longitudes de onda, las crestas de las ondas reflejadas en una cara de la  pompa de jabón coinciden con los valles de la onda reflejada en la otra cara. Los  colores correspondientes a dichas longitudes de onda están ausentes en la luz reflejada, que por lo tanto se muestra coloreada. 

La interferencia también puede producirse con partículas, debido a la dualidad  introducida por la mecánica cuántica. Un ejemplo famoso es el experimento llamado  de las dos rendijas (figura 4.2). Consideremos una fina pared con dos rendijas  paralelas. En un lado de la pared se coloca una fuente luminosa de un determinado  color, es decir, de una longitud de onda particular. La mayor parte de la luz chocará  contra la pared, pero una pequeña cantidad atravesará las rendijas. Supongamos,  entonces, que se sitúa una pantalla en el lado opuesto, respecto de la pared, de la  fuente luminosa. Cualquier punto de la pantalla recibirá luz de las dos rendijas. Sin  embargo, la distancia que tiene que viajar la luz desde la fuente a la pantalla, 
atravesando cada una de las rendijas, será, en general, diferente. Esto significará  que las ondas provenientes de las dos rendijas no estarán en fase entre sí cuando  lleguen a la pantalla: en algunos lugares las ondas se cancelarán entre sí, y en otros se reforzarán mutuamente. El resultado es un característico diagrama de franjas  luminosas y oscuras. 

Lo más notable es que se obtiene exactamente el mismo tipo de franjas si se  reemplaza la fuente luminosa por una fuente de partículas, tales como electrones, con  la misma velocidad (lo que significa que las ondas correspondientes poseen una  única longitud de onda). Ello resulta muy peculiar porque, si sólo se tiene una  rendija, no se obtienen franjas, sino simplemente una distribución uniforme de electrones a lo largo y ancho de la pantalla. Cabría pensar, por lo tanto, que la  apertura de la otra rendija simplemente aumentaría el número de electrones que  chocan en cada punto de la pantalla, pero, debido a la interferencia, este número  realmente disminuye en algunos lugares. Si los electrones se envían a través de las  rendijas de uno en uno, se esperaría que cada electrón pasara, o a través de una 

rendija, o a través de la otra, de forma que se comportaría justo igual a como si la  rendija por la que pasó fuera la única que existiese, produciendo una distribución  uniforme en la pantalla. En la realidad, sin embargo, aunque los electrones se envíen  de uno en uno, las franjas siguen apareciendo. Así pues, ¡cada electrón deber pasar  a través de las dos rendijas al mismo tiempo! 
El fenómeno de la interferencia entre partículas ha sido crucial para la comprensión  de la estructura de los átomos, las unidades básicas de la química y de la biología, y  los ladrillos a partir de los cuales nosotros, y todas las cosas a nuestro alrededor,  estamos formados. Al principio de este siglo se creyó que los átomos eran bastante  parecidos a los planetas girando alrededor del Sol, con los electrones (partículas de  electricidad negativa) girando alrededor del núcleo central, que posee electricidad  positiva. Se supuso que la atracción entre la electricidad positiva y la negativa  mantendría a los electrones en sus órbitas, de la misma manera que la atracción  gravitatoria entre el Sol y los planetas mantiene a éstos en sus órbitas. El problema  con este modelo residía en que las leyes de la mecánica y la electricidad predecían,  antes de que existiera la mecánica cuántica, que los electrones perderían energía y  caerían girando en espiral, hasta que colisionaran con el núcleo. Esto implicaría que  el átomo, y en realidad toda la materia, debería colapsarse rápidamente a un estado  de muy alta densidad. Una solución parcial a este problema la encontró el científico  danés Niels Bohr en 1913. Sugirió que quizás los electrones no eran capaces de  girar a cualquier distancia del núcleo central, sino sólo a ciertas distancias especificas. Si también se supusiera que sólo uno o dos electrones podían orbitar a  cada una de estas distancias, se resolvería el problema del colapso del átomo,  porque los electrones no podrían caer en espiral más allá de lo necesario, para llenar  las órbitas correspondientes a las menores distancias y energías. 
Este modelo explicó bastante bien la estructura del átomo más simple, el hidrógeno,  que sólo tiene un electrón girando alrededor del núcleo. Pero no estaba claro cómo  se debería extender la teoría a átomos más complicados. Además, la idea de un  conjunto limitado de órbitas permitidas parecía muy arbitraria. La nueva teoría de la  mecánica cuántica resolvió esta dificultad. Reveló que un electrón girando alrededor  del núcleo podría imaginarse como una onda, con una longitud de onda que 
dependía de su velocidad. Existirían ciertas órbitas cuya longitud correspondería a  un número entero (es decir, un número no fraccionario) de longitudes de onda del  electrón. Para estas órbitas las crestas de las ondas estarían en la misma posición  en cada giro, de manera que las ondas se sumarían: estas órbitas corresponderían a  las órbitas permitidas de Bohr. Por el contrario, para órbitas cuyas longitudes no  fueran un número entero de longitudes de onda, cada cresta de la onda sería  finalmente cancelada por un valle, cuando el electrón pasara de nuevo; estas órbitas  no estarían permitidas.

Un modo interesante de visualizar la dualidad onda-partícula es a través del método  conocido como suma sobre historias posibles, inventado por el científico norteamericano Richard Feynman. En esta aproximación, la partícula se supone que  no sigue una única historia o camino en el espacio-tiempo, como haría en una teoría  clásica, en el sentido de no cuántica. En vez de esto, se supone que la partícula va  de A a B a través de todos los caminos posibles. A cada camino se le asocia un par  de números: uno representa el tamaño de una onda y el otro representa la posición  en el ciclo (es decir, si se trata de una cresta o de un valle, por ejemplo). La  probabilidad de ir de A a B se encuentra sumando las ondas asociadas a todos los  caminos posibles. Si se compara un conjunto de caminos cercanos, en el caso  general, las fases o posiciones en el ciclo diferirán enormemente. Esto significa que  las ondas asociadas con estos caminos se cancelarán entre sí casi exactamente.  
Sin embargo, para algunos conjuntos de caminos cercanos, las fases no variarán  mucho de uno a otro; las ondas de estos caminos no se cancelarán. Dichos caminos  corresponden a las órbitas permitidas de Bohr. 
Con estas ideas, puestas en forma matemática concreta, fue relativamente sencillo  calcular las órbitas permitidas de átomos complejos e incluso de moléculas, que son  conjuntos de átomos unidos por electrones, en órbitas que giran alrededor de más  de un núcleo. Ya que la estructura de las moléculas, junto con las reacciones entre  ellas, son el fundamento de toda la química y la biología, la mecánica cuántica nos  permite, en principio, predecir casi todos los fenómenos a nuestro alrededor, dentro  de los límites impuestos por el principio de incertidumbre. (En la práctica, sin embargo, los cálculos que se requieren para sistemas que contengan a más de unos  pocos electrones son tan complicados que no pueden realizarse.) 
La teoría de la relatividad general de Einstein parece gobernar la estructura a gran  escala del universo. Es lo que se llama una teoría clásica, es decir, no tiene en  cuenta el principio de incertidumbre de la mecánica cuántica, como debería hacer  para ser consistente con otras teorías. La razón por la que esto no conduce a  ninguna discrepancia con la observación es que todos los campos gravitatorios, que  normalmente experimentamos, son muy débiles. Sin embargo, los teoremas sobre  las singularidades, discutidos anteriormente, indican que el campo gravitatorio  deberá ser muy intenso en, como mínimo, dos situaciones: los agujeros negros y el  big bang. En campos así de intensos, los efectos de la mecánica cuántica tendrán  que ser importantes. Así, en cierto sentido, la relatividad general clásica, al predecir  puntos de densidad infinita, predice su propia caída, igual que la mecánica clásica  (es decir, no cuántica) predijo su caída, al sugerir que los átomos deberían colapsarse hasta alcanzar una densidad infinita. Aún no tenemos una teoría consistente completa que unifique la relatividad general y la mecánica cuántica, pero sí que conocemos algunas de las características que debe poseer. Las consecuencias que éstas tendrían para los agujeros negros y el big bang se  describirán en capítulos posteriores. Por el momento, sin embargo, volvamos a los  intentos recientes de ensamblar las teorías parciales de las otras fuerzas de la  naturaleza en una única teoría cuántica unificada.

Capítulo 5 
LAS PARTÍCULAS ELEMENTALES  
Y LAS FUERZAS DE LA NATURALEZA 
Aristóteles creía que toda la materia del universo estaba compuesta por cuatro  elementos básicos: tierra, aire, fuego y agua. Estos elementos sufrían la acción de  dos fuerzas: la gravedad o tendencia de la tierra y del agua a hundirse, y la ligereza o  tendencia del aire y del fuego a ascender. Esta división de los contenidos del  
universo en materia y fuerzas aún se sigue usando hoy en día. 
También creía Aristóteles que la materia era continua, es decir, que un pedazo de  materia se podía dividir sin límite en partes cada vez más pequeñas: nunca se  tropezaba uno con un grano de materia que no se pudiera continuar dividiendo. Sin  embargo, unos pocos sabios griegos, como Demócrito, sostenían que la materia era  inherentemente granular y que todas las cosas estaban constituidas por un gran  número de diversos tipos diferentes de átomos. (La palabra átomo significa 
'indivisible', en griego.) Durante siglos, la discusión continuó sin ninguna evidencia  real a favor de cualesquiera de las posturas, hasta que en 1803, el químico y físico  británico John Dalton señaló que el hecho de que los compuestos químicos siempre  se combinaran en ciertas proporciones podía ser explicado mediante el 
agrupamiento de átomos para formar otras unidades llamadas moléculas. No  obstante, la discusión entre las dos escuelas de pensamiento no se zanjó de modo  definitivo a favor de los atomistas, hasta los primeros años de nuestro siglo. Una de  las evidencias físicas más importantes fue la que proporcionó Einstein. En un 
artículo escrito en 1905, unas pocas semanas antes de su famoso artículo sobre la  relatividad especial, Einstein señaló que el fenómeno conocido como movimiento  browniano -el movimiento irregular, aleatorio de pequeñas partículas de polvo suspendidas en un líquido- podía ser explicado por el efecto de las colisiones de los  átomos del líquido con las partículas de polvo. 
En aquella época ya había sospechas de que los átomos no eran, después de todo,  indivisibles. Hacía varios años que un fellow del Trinity College, de Cambridge, J. J.  Thomson, había demostrado la existencia de una partícula material, llamada electrón,  que tenía una masa menor que la milésima parte de la masa del átomo más ligero.  Él utilizó un dispositivo parecido al tubo de un aparato de televisión: un filamento  metálico incandescente soltaba los electrones, que, debido a que tienen una carga  eléctrica negativa, podían ser acelerados por medio de un campo eléctrico hacia una  pantalla revestida de fósforo. Cuando los electrones chocaban contra la pantalla, se  generaban destellos luminosos. Pronto se comprendió que estos electrones debían 

provenir de los átomos en sí. Y, en 1911, el físico británico Ernest Rutherford mostró,  finalmente, que los átomos de la materia tienen verdaderamente una estructura  interna: están formados por un núcleo extremadamente pequeño y con carga positiva, alrededor del cual gira un cierto número de electrones. Él dedujo esto  analizando el modo en que las partículas o¿, que son partículas con carga positiva  emitidas por átomos radioactivos, son desviadas al colisionar con los átomos. 
Al principio se creyó que el núcleo del átomo estaba formado por electrones y cantidades diferentes de una partícula con positiva llamada protón (que proviene del  griego y significa 'primero', porque se creía que era la unidad fundamental de la que  estaba hecha la materia). Sin embargo, en 1932, un colega de Rutherford, James  Chadwick, descubrió en Cambridge que el núcleo contenía otras partículas, llamadas neutrones, que tenían casi la misma masa que el protón, pero que no  poseían carga eléctrica. Chadwick recibió el premio Nobel por este descubrimiento,  y fue elegido master ['director'] de Gonville and Caius College, en Cambridge (el  colegio del que ahora soy fellow). Más tarde, dimitió como master debido a  desacuerdos con los fellows. Ha habido una amarga y continua disputa en el college  desde que un grupo de jóvenes fellows, a su regreso después de la guerra, 
decidieron por votación echar a muchos de los antiguos fellows de los puestos que  habían disfrutado durante mucho tiempo. Esto fue anterior a mi época; yo entré a  formar parte del college en 1965, al final de la amargura, cuando desacuerdos  similares habían forzado a otro master galardonado igualmente con el premio Nobel,  sir Nevill Mott, a dimitir. 
Hasta hace veinte años, se creía que los protones y los neutrones eran partículas  «elementales», pero experimentos en los que colisionaban protones con otros protones o con electrones a alta velocidad indicaron que, en realidad, estaban formados por partículas más pequeñas. Estas partículas fueron llamadas quarks por  
el físico de Caltech, Murray Gell-Mann, que ganó el premio Nobel en 1969 por su  trabajo sobre dichas partículas. El origen del nombre es una enigmática cita de  James Joyce: «¡Tres quarks para Muster Mark!» La palabra quark se supone que  debe pronunciarse como quart ['cuarto'], pero con una k al final en vez de una t, pero  normalmente se pronuncia de manera que rima con lark ['juerga']. 
Existe un cierto número de variedades diferentes de quarks: se cree que hay como  mínimo seis flavors ['sabores'], que llamamos up, down, strange, charmed, bottom, y  top ['arriba', 'abajo', 'extraño', 'encanto', 'fondo' y 'cima']. Cada flavor puede tener uno  de los tres posibles «colores», rojo, verde y azul. (Debe notarse que estos términos  son únicamente etiquetas: los quarks son mucho más pequeños que la longitud de  onda de la luz visible y, por lo tanto, no poseen ningún color en el sentido normal de la  palabra. Se trata solamente de que los físicos modernos parecen tener unas formas 

más imaginativas de nombrar a las nuevas partículas y fenómenos, ¡ya no se limitan  únicamente al griego!) Un protón o un neutrón están constituidos por tres quarks, uno  de cada color. Un protón contiene dos quarks up y un quark down; un neutrón  contiene dos down y uno up. Se pueden crear partículas constituidas por los otros  quarks (strange, charmed, bottom y top), pero todas ellas poseen una masa mucho  mayor y decaen muy rápidamente en protones y neutrones. 
Actualmente sabemos que ni los átomos, ni los protones y neutrones, dentro de ellos,  son indivisibles. Así la cuestión es: ¿cuáles son las verdaderas partículas elementales, los ladrillos básicos con los que todas las cosas están hechas? Dado  que la longitud de onda de la luz es mucho mayor que el tamaño de un átomo, no  podemos esperar «mirar» de manera normal las partes que forman un átomo.  Necesitamos usar algo con una longitud de onda mucho más pequeña. Como vimos  en el último capítulo, la mecánica cuántica nos dice que todas las partículas son en  realidad ondas, y que cuanto mayor es la energía de una partícula, tanto menor es la  longitud de onda de su onda correspondiente. Así, la mejor respuesta que se puede  dar a nuestra pregunta depende de lo alta que sea la energía que podamos comunicar a las partículas, porque ésta determina lo pequeña que ha de ser la  escala de longitudes a la que podemos mirar. Estas energías de las partículas se  miden normalmente en una unidad llamada electrón-voltio. (En el experimento de  Thomson con electrones, se vio que él usaba un campo eléctrico para acelerarlos.  La energía ganada por un electrón en un campo eléctrico de un voltio es lo que se  conoce como un electrón-voltio.) En el siglo xix, cuando las únicas energías de  partículas que la gente sabía cómo usar eran las bajas energías de unos pocos  electrón-voltios, generados por reacciones químicas tales como la combustión, se  creía que los átomos eran la unidad más pequeña. En el experimento de Rutherford,  las partículas alfa tenían energías de millones de electrón-voltios. Mas recientemente, hemos aprendido a usar los campos electromagnéticos para que nos  den energías de partículas que en un principio eran de millones de electrón-voltios y  que, posteriormente, son de miles de millones de electrón-voltios. De esta forma,  sabemos que las partículas que se creían «elementales» hace veinte años, están, de  hecho, constituidas por partículas más pequeñas. ¿Pueden ellas, conforme obtenemos energías todavía mayores, estar formadas por partículas aún más pequeñas? Esto es ciertamente posible, pero tenemos algunas razones teóricas  para creer que poseemos, o estamos muy cerca de poseer, un conocimiento de los  ladrillos fundamentales de la naturaleza. 
Usando la dualidad onda-partículas, discutida en el último capítulo, todo en el universo, incluyendo la luz y la gravedad, puede ser descrito en términos de partículas. Estas partículas tienen una propiedad llamada espín. Un modo de  imaginarse el espín es representando a las partículas como pequeñas peonzas 

girando sobre su eje. Sin embargo, esto puede inducir a error, porque la mecánica  cuántica nos dice que las partículas no tienen ningún eje bien definido. Lo que nos  dice realmente el espín de una partícula es cómo se muestra la partícula desde  distintas direcciones. Una partícula de espín 0 es como un punto: parece la misma  desde todas las direcciones (figura 5.1 a). Por el contrario, una partícula de espín 1  es como una flecha: parece diferente desde direcciones distintas (figura 5.1 b). Sólo  si uno la gira una vuelta completa (360 grados) la partícula parece la misma. Una  partícula de espín 2 es como una flecha con dos cabezas (figura 5.1 c): parece la  misma si se gira media vuelta (180 grados). De forma similar, partículas de espines  más altos parecen las mismas si son giradas una fracción más pequeña de una  vuelta completa. Todo esto parece bastante simple, pero el hecho notable es que  existen partículas que no parecen las mismas si uno las gira justo una vuelta: ¡hay  que girarlas dos vueltas completas! Se dice que tales partículas poseen espín 1/2. 
Todas las partículas conocidas de¡ universo se pueden dividir en dos grupos: partículas de espín 1/2, las cuales forman la materia del universo, y partículas de  espín 0, 1 y 2, las cuales, como veremos, dan lugar a las fuerzas entre las partículas  materiales. Las partículas materiales obedecen a lo que se llama el principio de  exclusión de Pan¡¡. Fue descubierto en 1925 por un físico austríaco, Wolfgang Pauli,  que fue galardonado con el premio Nobel en 1945 por dicha contribución. Él era el  prototipo de físico teórico: se decía que incluso su sola presencia en una ciudad  haría que allí los experimentos fallaran. El principio de exclusión de Pauli dice que  dos partículas similares no pueden existir en el mismo estado, es decir, que no  pueden tener ambas la misma posición y la misma velocidad, dentro de los límites  fijados por el principio de incertidumbre. El principio de exclusión es crucial porque  explica por qué las partículas materiales no colapsan a un estado de muy alta  densidad, bajo la influencia de las fuerzas producidas por las partículas de espín 0, 1  y 2: si las partículas materiales están casi en la misma posición, deben tener entonces velocidades diferentes, lo que significa que no estarán en la misma posición durante mucho tiempo. Si el mundo hubiera sido creado sin el principio de  exclusión, los quarks no formarían protones y neutrones independientes bien 
definidos. Ni tampoco éstos formarían, junto con los electrones, átomos independientes bien definidos. Todas las partículas se colapsarían formando una  «sopa» densa, más o menos uniforme.

Un entendimiento adecuado del electrón y de las otras partículas de espín 1/2 no  llegó hasta 1928, en que una teoría satisfactoria fue propuesta por Paul Dirac, quien  más tarde obtuvo la cátedra Lucasian de Matemáticas, de Cambridge (la misma  cátedra que Newton había obtenido y que ahora ocupo yo). La teoría de Dirac fue la  primera que era a la vez consistente con la mecánica cuántica y con la teoría de la  relatividad especial. Explicó matemáticamente por qué el electrón tenía espín 1/2,  es decir, por qué no parecía lo mismo si se giraba sólo una vuelta completa, pero sí  que lo hacía si se giraba dos vueltas. También predijo que el electrón debería tener  una pareja: el antielectrón o positrón. El descubrimiento del positrón en 1932  confirmó la teoría de Dirac y supuso el que se le concediera el premio Nobel de  física en 1933. Hoy en día sabemos que cada partícula tiene su antipartícula, con la  que puede aniquilarse. (En el caso de partículas portadoras de fuerzas, las antipartículas son las partículas mismas.) Podrían existir antimundos y antipersonas  enteros hechos de antipartículas. Pero, si se encuentra usted con su antiyó, ¡no le dé  la mano! Ambos desaparecerían en un gran destello luminoso. La cuestión de por  qué parece haber muchas más partículas que antipartículas a nuestro alrededor es  extremadamente importante, y volveré a ella a lo largo de este capítulo.

En mecánica cuántica, las fuerzas o interacciones entre partículas materiales, se  supone que son todas transmitidas por partículas de espín entero, 0, 1 o 2. Lo que  sucede es que una partícula material, tal como un electrón o un quark, emite una  partícula portadora de fuerza. El retroceso producido por esta emisión cambia la  velocidad de la partícula material. La partícula portadora de fuerza colisiona después con otra partícula material y es absorbida. Esta colisión cambia la 
velocidad de la segunda partícula, justo igual a como si hubiera habido una fuerza  entre las dos partículas materiales. 
Una propiedad importante de las partículas portadoras de fuerza es que no obedecen el principio de exclusión. Esto significa que no existe.un límite al número  de partículas que se pueden intercambiar, por lo que pueden dar lugar a fuerzas muy  intensas. No obstante, si las partículas portadoras de fuerza poseen una gran masa,  será difícil producirlas e intercambiarlas a grandes distancias. Así las fuerzas que  ellas transmiten serán de corto alcance. Se dice que las partículas portadoras de  fuerza, que se intercambian entre sí las partículas materiales, son partículas virtuales  porque, al contrario que las partículas «reales», no pueden ser descubiertas  directamente por un detector de partículas. Sabemos que existen, no obstante,  porque tienen un efecto medible: producen las fuerzas entre las partículas materiales.  Las partículas de espín 0, 1 o 2 también existen en algunas circunstancias como  partículas reales, y entonces pueden ser detectadas directamente. En este caso se  nos muestran como lo que un físico clásico llamaría ondas, tales como ondas luminosas u ondas gravitatorias. A veces pueden ser emitidas cuando las partículas  materiales interactúan entre sí, por medio de un intercambio de partículas virtuales  portadoras de fuerza. (Por ejemplo, la fuerza eléctrica repulsiva entre dos electrones  es debida al intercambio de fotones virtuales, que no pueden nunca ser detectados  directamente; pero, cuando un electrón se cruza con otro, se pueden producir fotones  reales, que detectamos como ondas luminosas.) 
Las partículas portadoras de fuerza se pueden agrupar en cuatro categorías, de  acuerdo con la intensidad de la fuerza que trasmiten y con el tipo de partículas con  las que interactúan. Es necesario señalar que esta división en cuatro clases es una  creación artificioso del hombre; resulta conveniente para la construcción de teorías  parciales, pero puede no corresponder a nada más profundo. En el fondo, la  mayoría de los físicos esperan encontrar una teoría unificada que explicará las cuatro  fuerzas, como aspectos diferentes de una única fuerza. En verdad, muchos dirían  que éste es el objetivo principal de la física contemporánea. Recientemente, se han  realizado con éxito diversos intentos de unificación de tres de las cuatro categorías  de fuerza, lo que describiré en el resto de este capítulo. La cuestión de la unificación  de la categoría restante, la gravedad, se dejará para más adelante.

La primera categoría es la fuerza gravitatoria. Esta fuerza es universal, en el sentido  de que toda partícula la experimenta, de acuerdo con su masa o energía. La  gravedad es la más débil, con diferencia, de las cuatro fuerzas; es tan débil que no la  notaríamos en absoluto si no fuera por dos propiedades especiales que posee:  puede actuar a grandes distancias, y es siempre atractiva. Esto significa que las  muy débiles fuerzas gravitatorias entre las partículas individuales de dos cuerpos  grandes, como la Tierra y el Sol, pueden sumarse todas y producir una fuerza total  muy significativa. Las otras tres fuerzas o bien son de corto alcance, o bien son a  veces atractivas y a veces repulsivas, de forma que tienden a cancelarse. Desde el  punto de vista mecano-cuántico de considerar el campo gravitatorio, la fuerza entre  dos partículas materiales se representa transmitida por una partícula de espín 2  llamada gravitón. Esta partícula no posee masa propia, por lo que la fuerza que  transmite es de largo alcance. La fuerza gravitatoria entre el Sol y la Tierra se  atribuye al intercambio de gravitones entre las partículas que forman estos dos cuerpos. Aunque las partículas intercambiadas son virtuales, producen ciertamente  un efecto medible: ¡hacen girar a la Tierra alrededor del Sol! Los gravitones reales  constituyen lo que los físicos clásicos llamarían ondas gravitatorias, que son muy  débiles, y tan difíciles de detectar que aún no han sido observadas. 
La siguiente categoría es la fuerza electromagnética, que interactúa con las partículas cargadas eléctricamente, como los electrones y los quarks, pero no con  las partículas sin carga, como los gravitones. Es mucho más intensa que la fuerza  gravitatoria: la fuerza electromagnética entre dos electrones es aproximadamente un  millón de billones de billones de billones (un 1 con cuarenta y dos ceros detrás) de  veces mayor que la fuerza gravitatoria. Sin embargo, hay dos tipos de carga  
eléctrica, positiva y negativa. La fuerza entre dos cargas positivas es repulsiva, al  igual que la fuerza entre dos cargas negativas, pero la fuerza es atractiva entre una  carga positiva y una negativa. Un cuerpo grande, como la Tierra o el Sol, contiene  prácticamente el mismo número de cargas positivas y negativas. Así, las fuerzas  atractiva y repulsiva entre las partículas individuales casi se cancelan entre sí, 
resultando una fuerza electromagnética neta muy débil. Sin embargo, a distancias  pequeñas, típicas de átomos y moléculas, las fuerzas electromagnéticas dominan.  La atracción electromagnética entre los electrones cargados negativamente y los  protones del núcleo cargados positivamente hace que los electrones giren alrededor  del núcleo del átomo, igual que la atracción gravitatoria hace que la Tierra gire  alrededor de¡ Sol. La atracción electromagnética se representa causada por el  intercambio de un gran número de partículas virtuales sin masa de espín 1, llamadas  fotones. De nuevo, los fotones que son intercambiados son partículas virtuales. No  obstante, cuando un electrón cambia de una órbita permitida a otra más cercana al  núcleo, se libera energía emitiéndose un fotón real, que puede ser observado como  luz visible por el ojo humano, siempre que posea la longitud de onda adecuada, o por  un detector de fotones, tal como una película fotográfica. Igualmente, si un fotón real  colisiona con un átomo, puede cambiar a un electrón de una órbita cercana al núcleo  a otra más lejana. Este proceso consume la energía del fotón, que, por lo tanto, es  absorbido.

La tercera categoría es la llamada fuerza nuclear débil, que es la responsable de la  radioactividad y que actúa sobre todas las partículas materiales de espín 1/2, pero  no sobre las partículas de espín 0, 1 o 2, tales como fotones y gravitones. La fuerza  nuclear débil no se comprendió bien hasta 1967, en que Abdus Salam, del Imperial  College de Londres, y Steven Weinberg, de Harvard, propusieron una teoría que  unificaba esta interacción con la fuerza electromagnética, de la misma manera que  Maxwell había unificado la electricidad y el magnetismo unos cien años antes.  Sugirieron que además del fotón había otras tres partículas de espín 1, conocidas  colectivamente como bosones vectoriales masivos, que transmiten la fuerza débil.  Estas partículas se conocen como W+ (que se lee W más), W- (que se lee W menos)  y Z0 (que se lee Z cero), y cada una posee una masa de unos 100 GeV (GeV es la  abreviatura de gigaelectrón-voltio, o mil millones de electrón-voltios). La teoría de  Weinberg-Salam propone una propiedad conocida como ruptura de simetría  espontánea. Esto quiere decir que lo que, a bajas energías, parece ser un cierto  número de partículas totalmente diferentes es, en realidad, el mismo tipo de partícula, sólo que en estados diferentes. A altas energías todas estas partículas se  comportan de manera similar. El efecto es parecido al comportamiento de una bola  de ruleta sobre la rueda de la ruleta. A altas energías (cuando la rueda gira  rápidamente) la bola se comporta esencialmente de una única manera, gira dando  vueltas una y otra vez. Pero conforme la rueda se va frenando, la energía de la bola  disminuye, hasta que al final la bola se para en uno de los treinta y siete casilleros de  la rueda. En otras palabras, a bajas energías hay treinta y siete estados diferentes  en los que la bola puede existir. Si, por algún motivo, sólo pudiéramos ver la bola a  bajas energías, entonces ¡pensaríamos que había treinta y siete tipos diferentes de  bolas! 
En la teoría de Weinberg-Salam, a energías mucho mayores de 100 GeV, las tres  nuevas partículas y el fotón se comportarían todas de una manera similar. Pero a  energías más bajas, que se dan en la mayoría de las situaciones normales, esta  simetría entre las partículas se rompería. W+, W- y Z0 adquirirían grandes masas,  haciendo que la fuerza que trasmiten fuera de muy corto alcance. En la época en  que Salam y Weinberg propusieron su teoría, poca gente les creyó y, al mismo  tiempo, los aceleradores de partículas no eran lo suficientemente potentes como  para alcanzar las energías de 100 GeV requeridas para producir partículas W+, W- o  Z0 reales. No obstante, durante los diez años siguientes, las tres predicciones de la  teoría a bajas energías concordaron tan bien con los experimentos que, en 1979,  Salam y Weinberg fueron galardonados con el premio Nobel de física, junto con  Sheldon Glashow, también de Harvard, que había sugerido una teoría similar de  unificación de las fuerzas electromagnéticas y nucleares débiles. El comité de los  premios Nobel se salvó del riesgo de haber cometido un error al descubrirse, en  1983 en el CERN (Centro Europeo para la Investigación Nuclear), las tres partículas 

con masa compañeras del fotón, y cuyas masas y demás propiedades estaban de  acuerdo con las predichas por la teoría. Carlo Rubbia, que dirigía el equipo de  varios centenares de físicos que hizo el descubrimiento, recibió el premio Nobel,  junto con Simon van der Meer, el ingeniero del CERN que desarrolló el sistema de  almacenamiento de antimateria empleado. (¡Es muy difícil realizar hoy en día una  contribución clave en física experimental a menos que ya se esté en la cima!) 
La cuarta categoría de fuerza es la interacción nuclear fuerte, que mantiene a los  quarks unidos en el protón y el neutrón, y a los protones y neutrones juntos en los  núcleos de los átomos. Se cree que esta fuerza es trasmitida por otra partícula de  espín 1, llamada gluón, que sólo interactúa consigo misma y con los quarks. La  interacción nuclear posee una curiosa propiedad llamada confinamiento: siempre  liga a las partículas en combinaciones tales que el conjunto total no tiene color. No  se puede tener un único quark aislado porque tendría un color (rojo, verde o azul).  Por el contrario, un quark rojo tiene que juntarse con un quark verde y uno azul por  medio de una «cuerda» de gluones (rojo + verde + azul = blanco). Un triplete así,  constituye un protón o un neutrón. Otra posibilidad es un par consistente en un quark  y un antiquark (rojo + antirrojo, o verde + antiverde, o azul + antiazul = blanco). Tales  combinaciones forman las partículas conocidas como mesones, que son inestables  porque el quark y el antiquark se pueden aniquilar entre sí, produciendo electrones y  otras partículas. Similarmente, el confinamiento impide que se tengan gluones aislados, porque los gluones en sí también tienen color. En vez de ello, uno tiene que  tener una colección de gluones cuyos colores se sumen para dar el color blanco.  Esta colección forma una partícula inestable llamada glueball ('bola de gluones'). 
El hecho de que el confinamiento nos imposibilite la observación de un quark o de un  gluón aislados podría parecer que convierte en una cuestión metafísica la noción  misma de considerar a los quarks y a los gluones como partículas. Sin embargo,  existe otra propiedad de la interacción nuclear fuerte, llamada libertad asintótica, que  hace que los conceptos de quark y de gluón estén bien definidos. A energías  
normales, la interacción nuclear fuerte es verdaderamente intensa y une a los quarks  entre sí fuertemente. Sin embargo, experimentos realizados con grandes aceleradores de partículas indican que a altas energías la interacción fuerte se hace  mucho menos intensa, y los quarks y los gluones se comportan casi como partículas  libres. La figura 5.2 muestra una fotografía de una colisión entre un protón de alta  energía y un antiprotón. En ella, se produjeron varios quarks casi libres, cuyas  estelas dieron lugar a los «chorros» que se ven en la fotografía.

Figura 5:2 
El éxito de la unificación de las fuerzas electromagnéticas y nucleares débiles produjo un cierto número de intentos de combinar estas dos fuerzas con la interacción nuclear fuerte, en lo que se han llamado teorías de gran unificación (o  TGU). Dicho nombre es bastante ampuloso: las teorías resultantes ni son tan grandes, ni están totalmente unificadas, pues no incluyen la gravedad. Ni siquiera  son realmente teorías completas, porque contienen un número de parámetros cuyos  
valores no pueden deducirse de la teoría, sino que tienen que ser elegidos de forma  que se ajusten a los experimentos. No obstante, estas teorías pueden constituir un  primer paso hacia una teoría completa y totalmente unificada. La idea básica de las  TGU es la siguiente: como se mencionó arriba, la interacción nuclear fuerte se hace  menos intensa a altas energías; por el contrario, las fuerzas electromagnéticas y  débiles, que no son asintóticamente libres, se hacen más intensas a altas energías.  A determinada energía muy alta, llamada energía de la gran unificación, estas tres  fuerzas deberían tener todas la misma intensidad y sólo ser, por tanto, aspectos  diferentes de una única fuerza. Las TGU predicen, además, que a esta energía las  diferentes partículas materiales de espín 112, como los quarks y los electrones,  también serían esencialmente iguales, y se conseguiría así otra unificación.

El valor de la energía de la gran unificación no se conoce demasiado bien, pero  probablemente tendría que ser como mínimo de mil billones de GeV. La generación  actual de aceleradores de partículas puede hacer colisionar partículas con energías  de aproximadamente 100 GeV, y están planeadas unas máquinas que elevarían  estas energías a unos pocos de miles de GeV. Pero una máquina que fuera lo  suficientemente potente como para acelerar partículas hasta la energía de la gran  unificación tendría que ser tan grande como el sistema solar, y sería difícil que  obtuviese financiación en la situación económica presente. Así pues, es imposible  comprobar las teorías de gran unificación directamente en el laboratorio. Sin embargo, al igual que en el caso de la teoría unificada de las interacciones electromagnética y débil, existen consecuencias a baja energía de la teoría que sí  pueden ser comprobadas. 
La más interesante de ellas es la predicción de que los protones, que constituyen  gran parte de la masa de la materia ordinaria, pueden decaer espontáneamente en  partículas más ligeras, tales como antielectrones. Esto es posible porque en la  energía de la gran unificación no existe ninguna diferencia esencial entre un quark y  un antielectrón. Los tres quarks que forman el protón no tienen normalmente la  energía necesaria para poder transformarse en antielectrones, pero muy ocasionalmente alguno de ellos podría adquirir suficiente energía para realizar la  transición, porque el principio de incertidumbre implica que la energía de los quarks  dentro del protón no puede estar fijada con exactitud. El protón decaería entonces.  La probabilidad de que un quark gane la energía suficiente para esa transición es  tan baja que probablemente tendríamos que esperar como mínimo un millón de  billones de billones de años (un 1 seguido de treinta ceros). Este período es más  largo que el tiempo transcurrido desde el big bang, que son unos meros diez mil  millones de años aproximadamente (un 1 seguido de diez ceros). Así, se podría  pensar que la posibilidad de desintegración espontánea del protón no se puede  medir experimentalmente. Sin embargo, uno puede aumentar las probabilidades de  detectar una desintegración, observando una gran cantidad de materia con un número elevadísimo de protones. (Si, por ejemplo, se observa un número de protones igual a 1 seguido de treinta y un ceros por un período de un año, se  esperaría, de acuerdo con la TGU más simple, detectar más de una desintegración  del protón.) 
Diversos experimentos de este tipo han sido llevados a cabo, pero ninguno ha  producido una evidencia definitiva sobre el decaimiento del protón o del neutrón. Un  experimento utilizó ocho mil toneladas de agua y fue realizado en la mina salada de  Morton, en Ohio (para evitar que tuvieran lugar otros fenómenos, causados por rayos  cósmicos, que podrían ser confundidos con la desintegración de protones). Dado  que no se observó ninguna desintegración de protones durante el experimento, se 

puede calcular que la vida media del protón debe ser mayor de diez billones de  billones de años (1 con treinta y un ceros). Lo que significa más tiempo que la vida  media predicha por la teoría de gran unificación más simple, aunque existen teorías  más elaboradas en las que las vidas medias predichas son mayores. Experimentos  todavía más sensibles, involucrando incluso mayores cantidades de materia, serán  necesarios para comprobar dichas teorías. 
Aunque es muy difícil observar el decaimiento espontáneo de protones, puede ser  que nuestra propia existencia sea una consecuencia del proceso inverso, la producción de protones, o más simplemente de quarks, a partir de una situación  inicial en la que no hubiese más que quarks y antiquarks, que es la manera más  natural de imaginar que empezó el universo. La materia de la Tierra está formada  principalmente por protones y neutrones, que a su vez están formados por quarks.  No existen antiprotones o antineutrones, hechos de antiquarks, excepto unos pocos  que los físicos producen en grandes aceleradores de partículas. Tenemos evidencia, a través de los rayos cósmicos, de que lo mismo sucede con la materia  de nuestra galaxia: no hay antiprotones o antineutrones, aparte de unos pocos que  se producen como pares partícula/antipartícula en colisiones de altas energías. Si  hubiera extensas regiones de antimateria en nuestra galaxia, esperaríamos observar  grandes cantidades de radiación proveniente de los límites entre las regiones de  materia y antimateria, en donde muchas partículas colisionarían con sus antipartículas, y se aniquilarían entre sí, desprendiendo radiación de alta energía. 
No tenemos evidencia directa de si en otras galaxias la materia está formada por  protones y neutrones o por antiprotones y antineutrones, pero tiene que ser o lo uno o  lo otro: no puede, haber una mezcla dentro de una misma galaxia, porque en ese  caso observaríamos de nuevo una gran cantidad de radiación producida por las  aniquilaciones. Por lo tanto, creemos que todas las galaxias están compuestas por  quarks en vez de por antiquarks; parece inverosímil que algunas galaxias fueran de  materia y otras de antimateria. 
¿Por qué debería haber santísimos más quarks que antiquarks? ¿Por qué no existe  el mismo número de ellos? Es ciertamente una suerte para nosotros que sus cantidades sean desiguales porque, si hubieran sido las mismas, casi todos los  quarks y antiquarks se hubieran aniquilado entre sí en el universo primitivo y hubiera  
quedado un universo lleno de radiación, pero apenas nada de materia. No habría  habido entonces ni galaxias, ni estrellas, ni planetas sobre los que la vida humana  pudiera desarrollarse. Afortunadamente, las teorías de gran unificación pueden proporcionarnos una explicación de por qué el universo debe contener ahora más  quarks que antiquarks, incluso a pesar de que empezara con el mismo número de  ellos. Como hemos visto, las TGU permiten a los quarks transformarse en

antielectrones a altas energías. También permiten el proceso inverso, la conversión  de antiquarks en electrones, y de electrones y antielectrones en antiquarks y quarks.  Hubo un tiempo, en los primeros instantes del universo, en que éste estaba tan  caliente que las energías de las partículas eran tan altas que estas transformaciones  podían tener lugar. ¿Pero por qué debería esto suponer la existencia de más quarks  que antiquarks? La razón es que las leyes de la física no son exactamente las  mismas para partículas que para antipartículas. 
Hasta 1956, se creía que las leyes de la física poseían tres simetrías independientes  llamadas C, P y T. La simetría C significa que las leyes son las mismas para  partículas y para antipartículas. La simetría P implica que las leyes son las mismas  para una situación cualquiera y para su imagen especular (la imagen especular de  una partícula girando hacia la derecha es la misma partícula, girando hacia la  izquierda). La simetría T significa que si se invirtiera la dirección del movimiento de  todas las partículas y antipartículas, el sistema volvería a ser igual a como fue antes:  en otras palabras, las leyes son las mismas en las direcciones hacia adelante y  hacia atrás del tiempo. 
En 1956, dos físicos norteamericanos, Tsung-Dao Lee y Chen Ning Yang, sugirieron  que la fuerza débil no posee de hecho la simetría P. En otras palabras, la fuerza débil  haría evolucionar el universo de un modo diferente a como evolucioba la imagen  especular del mismo. El mismo aiño, una colega, Chien-Shiung Wu, probó que las  predicciones de aquéllos eran correctas. Lo hizo alineando los núcleos de átomos  radioactivos en un campo magnético, de tal modo que todos girairan en la misma  dirección, y demostró que se liberaban más electrones en una dirección que en la  otra. Al año siguiente, Lee y Yang recibieron el premio Nobel por su idea. Se  encontró también que la fuerza débil no poseía la simetría C. Es decir, un universo  formado por antipartículas se comportaría de manera diferente al nuestro. Sin embargo, parecía que la fuerza débil sí poseía la simetría combinada CP. Es decir,  el universo evolucionaría de la misma manera que su imagen especular si, además,  cada partícula fuera cambiada por su antipartícula. Sin embargo, en 1964 dos  norteamericanos más, J. W. Cronin y Val Fitch, descubrieron que ni siquiera la  simetría CP se conservaba en la desintegración de ciertas partículas llamadas  mesones-K. Cronin y Fitch recibieron finalmente, en 1980, el premio Nobel por su  trabajo. (¡Se han concedido muchos premios por mostrar que el universo no es tan  simple como podíamos haber pensado!) 
Existe un teorema matemático según el cual cualquier teoría que obedezca a la  mecánica cuántica y a la relatividad debe siempre poseer la simetría combinada  CPT. En otras palabras, el universo se tendría que comportar igual si se reemplazaran las partículas por antipartículas, si se tomara la imagen especular y se 

invirtiera la dirección del tiempo. Pero Cronin y Fitch probaron que si se reemplazaban las partículas por antipartículas y se tomaba la imagen especular,  pero no se invertía la dirección del tiempo, entonces el universo no se comportaría igual. Las leyes de la física tienen que cambiar, por lo tanto, si se invierte la  dirección del tiempo: no poseen, pues, la simetría T. 
Ciertamente, el universo primitivo no posee la simetría T: cuando el tiempo avanza, el  universo se expande; si el tiempo retrocediera, el universo se contraería. Y dado que  hay fuerzas que no poseen la simetría T, podría ocurrir que, conforme el universo se  expande, estas fuerzas convirtieran más antielectrones en quarks que electrones en  antiquarks. Entonces, al expandirse y enfriarse el universo, los antiquarks se  aniquilarían con los quarks, pero, como habría más quarks que antiquarks, quedaría  un pequeño exceso de quarks, que son los que constituyen la materia que vemos hoy  en día y de la que estamos hechos. Así, nuestra propia existencia podría ser vista  como una confirmación de las teorías de gran unificación, aunque sólo fuera una  confirmación únicamente cualitativa; las incertidumbres son tan grandes que no se  puede predecir el número de quarks que quedarían después de la aniquilación, o  incluso si serían los quarks o los antiquarks los que permanecerían. (Si hubiera  habido un exceso de antiquarks, sería lo mismo, pues habríamos llamado antiquarks  a los quarks, y quarks a los antiquarks.) 
Las teorías de gran unificación no incluyen a la fuerza de la gravedad. Lo cual no  importa demasiado, porque la gravedad es tan débil que sus efectos pueden normalmente ser despreciados cuando estudiamos partículas o átomos. Sin embargo, el hecho de que sea a la vez de largo alcance y siempre atractiva significa  que sus efectos se suman. Así, para un número de partículas materiales suficientemente grande, las fuerzas gravitatorias pueden dominar sobre todas las  demás. Por ello, la gravedad determina la evolución del universo. Incluso para  objetos del tamaño de una estrella, la fuerza atractiva de la gravedad puede dominar  sobre el resto de las fuerzas y hacer que la estrella se colapse. Mi trabajo en los  años setenta se centró en los agujeros negros que pueden resultar de un colapso  estelar y en hs intensos campos gravitatorios existentes a su alrededor. Fue eso lo  que nos condujo a las primeras pistas de cómo las teorías de la mecánica cuántica y  de la relatividad general podrían relacionarse entre sí: un vislumbre de la forma que  tendría una venidera teoría cuántica de la gravedad.

Capítulo 6 
LOS AGUJEROS NEGROS 
El término agujero negro tiene un origen muy reciente. Fue acuñado en 1969 por el  científico norteamericano John Wheeler como la descripción gráfica de una idea que  se remonta hacia atrás un mínimo de doscientos años, a una época en que había  dos teorías sobre la luz: una, preferida por Newton, que suponía que la luz estaba  compuesta por partículas, y la otra que asuma que estaba formada por ondas. Hoy  en día, sabemos que ambas teorías son correctas. Debido a la dualidad onda/  corpúsculo de la mecánica cuántica, la luz puede ser considerada como una onda y  como una partícula. En la teoría de que la luz estaba formada por ondas, no  quedaba claro como respondería ésta ante la gravedad. Pero si la luz estaba  compuesta por partículas, se podría esperar que éstas fueran afectadas por la  gravedad del mismo modo que lo son las balas, los cohetes y los planetas. Al  principio, se pensaba que las partículas de luz viajaban con infinita rapidez, de forma  que la gravedad no hubiera sido capaz de frenarías, pero el descubrimiento de  Roemer de que la luz viaja a una velocidad finita, significó el que la gravedad pudiera  tener un efecto importante sobre la luz. 
Bajo esta suposición, un catedrático de Cambridge, John Michell, escribió en 1783  un artículo en el Philosophical Transactions of the Royal Society of London en el  que señalaba que una estrella que fuera suficientemente masiva y compacta tendría  un campo gravitatorio tan intenso que la luz no podría escapar: la luz emitida desde  la superficie de la estrella sería arrastrada de vuelta hacia el centro por la atracción  gravitatoria de la estrella, antes de que pudiera llegar muy lejos. Michell sugirió que  podría haber un gran número de estrellas de este tipo. A pesar de que no seríamos  capaces de verlas porque su luz no nos alcanzaría, sí notaríamos su atracción gravitatoria. Estos objetos son los que hoy en día llamamos agujeros negros, ya que  esto es precisamente lo que son: huecos negros en el espacio. Una sugerencia  similar fue realizada unos pocos añ¿)s después por el científico francés marqués de  Laplace, parece ser que independientemente de Michell. Resulta bastante interesante que Laplace sólo incluyera esta idea en la primera y la segunda 
ediciones de su libro El sistema del mundo, y no la incluyera en las ediciones  posteriores. Quizás decidió que se trataba de una idea disparatada. (Hay que tener  en cuenta también que la teoría corpuscular de la luz cayó en desuso durante el siglo  xix; parecía que todo se podía explicar con la teoría ondulatorio, y, de acuerdo con  ella, no estaba claro si la luz sería afectada por la gravedad.)

De hecho, no es realmente consistente tratar la luz como las balas en la teoría de la  gravedad de Newton, porque la velocidad de la luz es fija. (Una bala disparada hacia  arriba desde la Tierra se irá frenando debido a la gravedad y, finalmente, se parará y  caerá; un fotón, sin embargo, debe continuar hacia arriba con velocidad constante.  ¿Cómo puede entonces afectar la gravedad newtoniana a la luz?) No apareció una  teoría consistente de cómo la gravedad afecta a la luz hasta que Einstein propuso la  relatividad general, en 1915. E incluso entonces, tuvo que transcurrir mucho tiempo  antes de que se comprendieran las ¡aplicaciones de la teoría acerca de las estrellas  masivas. 
Para entender cómo se podría formar un agujero negro, tenemos que tener ciertos  conocimientos acerca del ciclo vital de una estrella. Una estrella se forma cuando  una gran cantidad de gas, principalmente hidrógeno, comienza a colapsar sobre sí  mismo debido a su atracción gravitatoria. Conforme se contrae, sus átotnos empiezan a colisionar entre sí, cada vez con mayor frecuencia y a mayores velocidades: el gas se calienta. Con el tiempo, el gas estará tan caliente que cuando  
los átomos de hidrógeno choquen ya no saldrán rebotados, sino que se fundirán  formando helio. El calor desprendido por la reacción, que es como una explosión  controlada de una bomba de hidrógeno, hace que la estrella brille. Este calor  adicional también aumenta la presión del gas hasta que ésta es suficiente para  equilibrar la atracción gravitatoria, y el gas deja de contraerse. Se parece en cierta  medida a un globo. Existe un equilibrio entre la presión del aire de dentro, que trata  de hacer que el globo se hinche, y la tensión de la goma, que trata de disminuir el  tamaño del globo. Las estrellas permanecerán estables en esta forma por un largo  período, con el calor de las reacciones nucleares equilibrando la atracción gravitatoria. Finalmente, sin embargo, la estrella consumirá todo su hidrógeno y los  otros combustibles nucleares. Paradójicamente, cuanto más combustible posee una  estrella al principio, más pronto se le acaba. Esto se debe a que cuanto más masiva  
es la es estrella más caliente tiene que estar para contrarrestar la atracción gravitatoria, y, cuanto mas caliente está, más rápidamente utiliza su combustible.  Nuestro Sol tiene probablemente suficiente combustible para otros cinco mil millones  de años aproximadamente, pero estrellas más masivas pueden gastar todo su combustible en tan sólo cien millones de años, mucho menos que la edad del universo. Cuando una estrella se queda sin combustible, empieza a enfriarse y por lo  tanto a contraerse. Lo que puede sucederle a partir de ese momento sólo se  empezó a entender al final de los años veinte. 
En 1928, un estudiante graduado indio, Subrahmanyan Chandrasekhar, se embarcó  hacia Inglaterra para estudiar en Canibridge con el astrónomo británico sir Arthur  Eddington, un experto en relatividad general. (Según algunas fuentes, un periodista le  dijo a Eddington, al principio de los años veinte, que había oído que había sólo tres 

personas en el mundo que entendieran la relatividad general. Eddington hizo una  pausa, y luego replicó: «Estoy tratando de pensar quién es la tercera persona».)  
Durante su viaje desde la India, Chandrasekhar calculó lo grande que podría llegar a  ser una estrella que fuera capaz de soportar su propia gravedad, una vez que hubiera  gastado todo su combustible. La idea era la siguiente: cuando la estrella se reduce  en tamaño, las partículas materiales están muy cerca unas de otras, y así, de  acuerdo con el principio de exclusión de Pauli, tienen que tener velocidades muy  diferentes. Esto hace que se alejen unas de otras, lo que tiende a expandir a la  estrella. Una estrella puede, por lo tanto, mantenerse con un radio constante, debido  a un equilibrio entre la atracción de la gravedad y la repulsión que surge del principio  de exclusión, de la misma manera que antes la gravedad era compensada por el  calor. 
Chandrasekhar se dio cuenta, sin embargo, de que existe un límite a la repulsión que  el principio de exclusión puede proporcionar. La teoría de la relatividad limita la  diferencia máxima entre las velocidades de las partículas materiales de la estrella a  la velocidad de la luz. Esto significa que cuando la estrella fuera suficientemente  densa, la repulsión debida al principio de exclusión sería menor que la atracción de  la gravedad. Chandrasekhar calculó que una estrella fría de más de 
aproximadamente una vez y media la masa del Sol no sería capaz de soportar su  propia gravedad. (A esta masa se le conoce hoy en día como el límite de Chandrasekhar.) Un descubrimiento similar fue realizado, casi al mismo tiempo, por  el científico ruso Lev Davidovich Landau. 
Todo esto tiene serias aplicaciones en el destino último de las estrellas masivas. Si  una estrella posee una masa menor que el límite de Chandrasekhar, puede finalmente cesar de contraerse y estabilizarse en un posible estado final, como una  estrella «enana blanca», con un radio de unos pocos miles de kilómetros y una  densidad de decenas de toneladas por centímetro cúbico. Una enana blanca se  sostiene por la repulsión, debida al principio de exclusión entre los electrones de su  materia. Se puede observar un gran número de estas estrellas enanas blancas; una  de las primeras que se descubrieron fue una estrella que está girando alrededor de  Sirio, la estrella más brillante en el cielo nocturno. 
Landau señaló que existía otro posible estado final para una estrella, también con  una masa límite de una o dos veces la masa del Sol, pero mucho más pequeña  incluso que una enana blanca. Estas estrellas se mantendrían gracias a la repulsión  debida al principio de exclusión entre neutrones y protones, en vez de entre electrones. Se les llamó por eso estrellas de neutrones. Tendrían un radio de unos  quince kilómetros aproximadamente y una densidad de decenas de millones de 

toneladas por centímetro cúbico. En la época en que fueron predichas, no había  forma de poder observarlas; no fueron detectadas realmente hasta mucho después. 
Estrellas con masas superiores al límite de Chandrasekhar tienen, por el contrario,  un gran problema cuando se les acaba el combustible. En algunos casos consiguen  explotar, o se las arreglan para desprenderse de la suficiente materia como para  reducir su peso por debajo del límite y evitar así un catastrófico colapso gravitatorio;  pero es difícil pensar que esto ocurra siempre, independientemente de lo grande que  sea la estrella. ¿Cómo podría saber la estrella que tenía que perder peso? E incluso  si todas las estrellas se las arreglaran para perder la masa suficiente como para  evitar el colapso, ¿qué sucedería si se añadiera más masa a una enana blanca o a  una estrella de neutrones, de manera que se sobrepasara el límite? ¿Se colapsaría  alcanzando una densidad infinita? Eddington se asombró tanto por esta conclusión  que rehusó creerse el resultado de Chandrasekhar. Pensó que era simplemente  imposible que una estrella pudiera colapsarse y convertirse en un punto. Este fue el  criterio de la mayoría de los científicos: el mismo Einstein escribió un artículo en el  que sostenía que las estrellas no podrían encogerse hasta tener un tamaño nulo. La  hostilidad de otros científicos, en particular de Eddington, su antiguo profesor y principal autoridad en la estructura de las estrellas, persuadió a Chandrasekhar a  abandonar esta línea de trabajo y volver su atención hacia otros problemas de  astronomía, tales como el movimiento de los grupos de estrellas. Sin embargo,  cuando se le otorgó el premio Nobel en 1-983, fue, al menos en parte, por sus  primeros trabajos sobre la masa límite de las estrellas frías. 
Chandrasekhar había demostrado que el principio de exclusión no podría detener el  colapso de una estrella más masiva que el límite de Chandrasekhar, pero el problema de entender qué es lo que le sucedería a tal estrella, de acuerdo con la  relatividad general, fue resuelto por primera vez por un joven norteamericano, Robert  Oppenheimer, en 1939. Su resultado, sin embargo, sugería que no habría 
consecuencias observables que pudieran ser detectadas por un telescopio de su  época. Entonces comenzó la segunda guerra mundial y el propio Oppenheimer se  vio involucrado en el proyecto de la bomba atómica. Después de la guerra, el  problema del colapso gravitatorio fue ampliamente olvidado, ya que la mayoría de  los científicos se vieron atrapados en el estudio de lo que sucede a escala atómica y  nuclear. En los años sesenta, no obstante, el interés por los problemas de gran  escala de la astronomía y la cosmología fue resucitado a causa del aumento en el  número y categoría de las observaciones astronómicas, ocasionado por la aplicación de la tecnología moderna. El trabajo de Oppenheimer fue entonces redescubierto y adoptado por cierto número de personas.
