{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import (\n",
    "    load_book, \n",
    "    format_text_with_line_breaks, \n",
    "    extract_keywords_from_fragments, \n",
    "    generate_wordcloud,\n",
    "    initialize_vector_database,\n",
    "    insert_text_fragments,\n",
    "    hybrid_search,\n",
    "    reciprocal_rank_fusion,\n",
    "    answer_question_with_context_streaming,\n",
    "    process_text_into_chunks\n",
    ")\n",
    "from text_chunking.SemanticClusterVisualizer import SemanticClusterVisualizer\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import markdown\n",
    "\n",
    "logging.getLogger('langchain').setLevel(logging.ERROR)\n",
    "logging.getLogger('requests').setLevel(logging.ERROR)\n",
    "logging.getLogger('openai').setLevel(logging.ERROR)\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*force_all_finite.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*n_jobs value.*overridden to 1 by setting random_state.*\")\n",
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.WARNING)\n",
    "\n",
    "for module in [\"sklearn\", \"umap\"]:\n",
    "    logging.getLogger(module).setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDL y preparación de Postgres DB\n",
    "\n",
    "La base de datos PostgreSQL funciona como el almacén central de nuestro sistema. Imagina una biblioteca muy organizada donde guardamos tanto los textos completos como representaciones matemáticas de su significado.\n",
    "\n",
    "En esta etapa:\n",
    "\n",
    "- Configuramos PostgreSQL con una extensión especial llamada \"pgvector\" que le permite entender matemáticamente el significado de fragmentos de textos\n",
    "- Creamos una tabla llamada \"chunks\" (fragmentos) para almacenar:\n",
    "  * El texto original\n",
    "  * Su representación matemática (vectores)\n",
    "  * Un formato especial para búsquedas de palabras exactas\n",
    "  * Información adicional como palabras clave\n",
    "\n",
    "También implementamos índices, que son como los sistemas de clasificación en una biblioteca. Estos nos permiten encontrar rápidamente información sin tener que revisar todo el contenido, usando tanto el significado como las palabras exactas.\n",
    "\n",
    "Es similar a preparar un terreno y construir los cimientos antes de levantar una casa: estamos creando la infraestructura básica donde todo lo demás se apoyará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = initialize_vector_database(\n",
    "    host=\"localhost\",\n",
    "    port=5432,\n",
    "    dbname=\"workshop_rag\",\n",
    "    user=\"postgres\",\n",
    "    password=\"dev.2m\",\n",
    "    vector_dimensions= 1024 # Nuestro modelo de embeddings tiene 1024 dimensiones (open AI tiene 1536)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesta de libro- Texto y pre-fragmentado\n",
    "\n",
    "En esta fase, nuestro sistema \"lee\" el libro y lo prepara para un procesamiento más avanzado, como un chef que prepara los ingredientes antes de cocinar.\n",
    "\n",
    "El proceso incluye:\n",
    "\n",
    "- **Cargar el libro**: Leemos el archivo de texto completo, asegurándonos de que se interpretan correctamente los caracteres especiales del español (como acentos y eñes).\n",
    "\n",
    "- **Fragmentación inicial del texto**: Organizamos el texto en párrafos simples de manera limpia y consistente, respetando la estructura natural del libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = load_book(\"./book.txt\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "semantic_chunker = SemanticClusterVisualizer(\n",
    "    api_key= \"123\", \n",
    "    llm_model='gpt-4o',\n",
    "    base_url_llm= \"http://localhost:3000/v1\",\n",
    "    base_url_embeddings= \"http://localhost:3001/v1\",\n",
    "    embeddings_model= \"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# split the document into chunks\n",
    "original_split_texts = process_text_into_chunks(\n",
    "    full_text\n",
    ")\n",
    "print(f\"Original split texts: {len(original_split_texts)}\\n\\n\")\n",
    "print(format_text_with_line_breaks(\"\\n\\n\".join(original_split_texts[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-proceso para búsquedas semánticas\n",
    "\n",
    "Aquí es donde el sistema aprende a \"entender\" el significado del texto, no solo las palabras exactas.\n",
    "\n",
    "Imagina que traduces cada fragmento de texto a un idioma universal matemático. Este componente:\n",
    "\n",
    "- **Crea \"embeddings\"**: Convierte texto en largas listas de números (vectores) donde textos con significados similares tendrán números similares, incluso si usan palabras diferentes.\n",
    "  * Por ejemplo, \"estoy feliz\" y \"me siento contento\" tendrían representaciones numéricas parecidas aunque usen palabras distintas.\n",
    "\n",
    "- **Realiza chunking semántico**: Este proceso es fundamental para preservar el significado:\n",
    "  * Toma los fragmentos básicos generados en la etapa anterior\n",
    "  * Analiza la similitud semántica entre fragmentos consecutivos usando sus embeddings\n",
    "  * Agrupa fragmentos que están conceptualmente relacionados\n",
    "  * Evita cortes en medio de explicaciones, definiciones o argumentos importantes\n",
    "  * Prioriza mantener juntos párrafos que complementan una misma idea\n",
    "\n",
    "- **Preserva contextos completos**: A diferencia del fragmentado simple por cantidad de caracteres o párrafos:\n",
    "  * Identifica secciones temáticas completas\n",
    "  * Evita separar elementos que dependen unos de otros para su comprensión\n",
    "\n",
    "- **Equilibra tamaño y coherencia**: Busca un balance óptimo para que los fragmentos:\n",
    "  * Sean lo suficientemente grandes para contener ideas completas\n",
    "  * No tan extensos que diluyan el significado central\n",
    "  * Mantengan relaciones causa-efecto dentro del mismo fragmento\n",
    "\n",
    "Este proceso emula cómo los humanos entendemos textos: agrupamos naturalmente ideas relacionadas y mantenemos el hilo conductor de un argumento, incluso cuando abarca varios párrafos. El chunking semántico es crucial para que el sistema pueda ofrecer respuestas coherentes y completas, evitando información fragmentada o sacada de contexto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run embeddings\n",
    "original_split_text_embeddings = semantic_chunker.embed_original_document_splits(original_split_texts)\n",
    "\n",
    "# generate breakpoints, use length threshold to decide which \n",
    "# sections to further subdivide \n",
    "breakpoints, semantic_groups = semantic_chunker.generate_breakpoints(\n",
    "    original_split_texts,\n",
    "    original_split_text_embeddings,\n",
    "    percentile_threshold= 0.9,\n",
    "    length_threshold= 5000,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(f'breakpoints: {breakpoints}')\n",
    "print(f'original_split_texts #: {len(original_split_texts)}')\n",
    "print(f'semantic_groups #: {len(semantic_groups)}')\n",
    "print(f'avg word count of semantic_groups: {np.mean([len(x.split()) for x in semantic_groups])}')\n",
    "print(f'semantic_groups: {format_text_with_line_breaks(semantic_groups[2])}')\n",
    "\n",
    "# embed the groups that have been made from the breakpoints\n",
    "semantic_group_embeddings = semantic_chunker.embed_semantic_groups(semantic_groups)\n",
    "\n",
    "# cluster the groups\n",
    "splits_df, semantic_group_clusters = semantic_chunker.vizualize_semantic_groups(\n",
    "    semantic_groups,\n",
    "    semantic_group_embeddings,\n",
    "    n_clusters= 8\n",
    ")\n",
    "\n",
    "# generate cluster summaries\n",
    "cluster_summaries = semantic_chunker.generate_cluster_labels(\n",
    "    semantic_group_clusters, plot= True\n",
    ")\n",
    "\n",
    "# generate cluster bounds\n",
    "semantic_cluster_bounds = semantic_chunker.split_visualizer.plot_corpus_and_clusters(\n",
    "    splits_df, cluster_summaries\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-proceso para búsquedas por coincidencias de la información\n",
    "\n",
    "Este componente se enfoca en las palabras más importantes de cada texto, como un estudiante que subraya los términos clave en sus apuntes.\n",
    "\n",
    "El proceso incluye:\n",
    "\n",
    "- **Extracción de palabras clave**: Usa un algoritmo llamado BM25 (similar al que usa Google) para identificar qué palabras son realmente importantes en cada fragmento.\n",
    "  * Prioriza palabras que aparecen mucho en un fragmento específico pero poco en el resto del libro.\n",
    "  * Por ejemplo, en un capítulo sobre \"fotosíntesis\", esta palabra sería muy relevante si apenas aparece en otros capítulos.\n",
    "\n",
    "- **Procesamiento eficiente**: Distribuye este trabajo entre varios núcleos del procesador para hacerlo más rápido, como tener varios asistentes analizando diferentes párrafos simultáneamente.\n",
    "\n",
    "- **Visualización de palabras clave**: Crea \"nubes de palabras\" donde los términos más importantes aparecen más grandes, ofreciendo una representación visual rápida del contenido.\n",
    "\n",
    "Estas palabras clave complementan la búsqueda semántica, permitiendo encontrar fragmentos que contienen exactamente los términos que buscamos, no solo conceptos similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer keywords de cada fragmento (10 por defecto)\n",
    "key_words_for_semantic_groups = extract_keywords_from_fragments(semantic_groups, full_text, top_n= 30)\n",
    "\n",
    "# Acceder a los keywords de un fragmento específico\n",
    "keywords_primer_fragmento = key_words_for_semantic_groups[0]\n",
    "generate_wordcloud(key_words_for_semantic_groups, \"Palabras claves extraídas de los fragmentos de texto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de índice híbrido para recuperación de información\n",
    "\n",
    "En esta fase construimos un sistema de búsqueda dual que combina lo mejor de dos mundos: la búsqueda por significado (semántica) y la búsqueda por palabras exactas.\n",
    "\n",
    "El proceso es como crear un índice doble para un libro:\n",
    "\n",
    "- **Almacenamiento completo**: Para cada fragmento de texto guardamos:\n",
    "  * El texto original completo\n",
    "  * Su traducción matemática (vector de embeddings)\n",
    "  * Las palabras clave importantes\n",
    "  * Información adicional como cuándo fue procesado\n",
    "\n",
    "- **Procesamiento optimizado**: Trabajamos con grupos de fragmentos a la vez para ser eficientes.\n",
    "  * Es similar a enviar documentos a archivar en lotes, en lugar de uno por uno.\n",
    "  * El sistema informa del progreso para saber cuánto falta.\n",
    "\n",
    "- **Indexación automática**: La base de datos crea automáticamente sistemas de búsqueda rápida:\n",
    "  * Uno para encontrar textos con significado similar (como un índice temático)\n",
    "  * Otro para encontrar palabras específicas (como un índice alfabético)\n",
    "\n",
    "Este componente es como un bibliotecario experto que puede encontrar libros tanto por su tema general como por palabras específicas que contienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserted_count = insert_text_fragments(\n",
    "    connection= connection,\n",
    "    text_fragments= semantic_groups,\n",
    "    keywords_lists= key_words_for_semantic_groups,\n",
    "    api_key= \"123\",\n",
    "    base_url_embeddings= \"http://localhost:3001/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperación híbrida\n",
    "\n",
    "Este es el componente que realmente realiza las búsquedas combinando dos enfoques potentes:\n",
    "\n",
    "- **Búsqueda semántica** (por significado):\n",
    "  * Convierte la pregunta del usuario en un vector matemático de embeddings\n",
    "  * Encuentra fragmentos cuyo vector es similar, aunque usen palabras diferentes\n",
    "  * Es como encontrar recetas similares aunque usen términos culinarios distintos\n",
    "\n",
    "- **Búsqueda textual** (por palabras):\n",
    "  * Extrae las palabras importantes de la pregunta\n",
    "  * Busca fragmentos que contengan exactamente esas palabras\n",
    "  * Es como buscar recetas que mencionen específicamente \"chocolate\" o \"sin gluten\"\n",
    "\n",
    "El sistema combina inteligentemente ambos resultados:\n",
    "  * Elimina duplicados (textos encontrados por ambos métodos)\n",
    "  * Da preferencia a fragmentos encontrados tanto por significado como por palabras exactas\n",
    "  * Ajusta la cantidad de resultados según lo que encuentre\n",
    "\n",
    "Es como tener dos asistentes de investigación: uno experto en entender conceptos generales y otro especializado en encontrar términos específicos, trabajando juntos para darte la mejor respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Tienes información sobre el experimento de la doble rendija'\n",
    "results = hybrid_search(\n",
    "    connection= connection,\n",
    "    query_text= query,\n",
    "    api_key= \"123\",\n",
    "    base_url_embeddings= \"http://localhost:3001/v1\",\n",
    "    top_k= 20\n",
    ")\n",
    "print(len(results))\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Componente de Ranking por relevancia (Reciprocal Rank Fusion)\n",
    "\n",
    "Este componente decide qué fragmentos son realmente los más relevantes para la pregunta, como un juez que evalúa las respuestas de diferentes expertos.\n",
    "\n",
    "El proceso, llamado Reciprocal Rank Fusion (RRF), funciona así:\n",
    "\n",
    "- **Combina los rankings de ambos métodos de búsqueda**:\n",
    "  * Da importancia a la posición en cada ranking (los primeros lugares valen más)\n",
    "  * También considera cuán relevante era cada fragmento según cada método\n",
    "  * Usa una fórmula matemática especial para que ningún método domine completamente\n",
    "\n",
    "- **Premia la consistencia**:\n",
    "  * Si un fragmento aparece en ambos métodos de búsqueda, recibe un 20% de puntuación extra\n",
    "  * Esto refleja que probablemente es más relevante si dos métodos diferentes lo encontraron\n",
    "\n",
    "- **Reordena los resultados**:\n",
    "  * Crea una lista final ordenada por relevancia combinada\n",
    "  * Mantiene solo los mejores resultados (top_k)\n",
    "\n",
    "Este componente es como un director de orquesta que toma las contribuciones de diferentes músicos (métodos de búsqueda) y las armoniza en una sola pieza coherente, destacando las partes donde hay mayor acuerdo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_results = reciprocal_rank_fusion(results, top_k= 7, k_constant= 60.0)\n",
    "pd.DataFrame(reranked_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "\n",
    "El \"prompting\" es como dar instrucciones precisas a un asistente inteligente para que responda preguntas usando solo la información proporcionada.\n",
    "\n",
    "Este componente:\n",
    "\n",
    "- **Crea instrucciones claras (prompts)** para el modelo de lenguaje:\n",
    "  * Incluye la pregunta original del usuario\n",
    "  * Proporciona los fragmentos relevantes encontrados, numerados como \"Documento 1\", \"Documento 2\", etc.\n",
    "  * Da instrucciones específicas: \"Responde solo usando la información proporcionada\"\n",
    "\n",
    "- **Formatea la información adecuadamente**:\n",
    "  * Organiza el contexto de manera estructurada\n",
    "  * Asegura que el modelo pueda distinguir claramente entre diferentes fragmentos\n",
    "\n",
    "Este componente es como un traductor entre los resultados de la búsqueda y el modelo de lenguaje, asegurándose de que este último entienda exactamente qué información debe usar y cómo responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Eres un asistente virtual especializado cuya función es proporcionar información basándote EXCLUSIVAMENTE \n",
    "en la base de conocimientos que se te ha proporcionado. Tu misión es:\n",
    "\n",
    "1. Analizar cuidadosamente cada consulta del usuario.\n",
    "2. Buscar información relevante ÚNICAMENTE dentro de la base de conocimientos proporcionada.\n",
    "3. Responder de manera detallada y estructurada utilizando formato Markdown.\n",
    "4. NO utilizar conocimientos propios, especulaciones o información externa que no esté explícitamente incluida en la base de conocimientos.\n",
    "5. Estructurar tus respuestas con títulos, subtítulos, listas y énfasis cuando sea apropiado para mejorar la legibilidad.\n",
    "\n",
    "## Casos especiales:\n",
    "\n",
    "Si la base de conocimientos NO contiene información suficiente para responder a la consulta del usuario:\n",
    "1. Indica claramente: \"No dispongo de información suficiente en mi base de conocimientos para responder a esta consulta específica.\"\n",
    "2. Ofrece información sobre un tema relacionado que SÍ esté presente en la base de conocimientos, introduciendo con: \"Sin embargo, puedo ofrecerte información sobre [tema relacionado] que podría ser de tu interés:\"\n",
    "\n",
    "## Formato de respuesta:\n",
    "\n",
    "Utiliza Markdown para estructurar tus respuestas:\n",
    "- Usa `#` para títulos principales\n",
    "- Usa `##` para subtítulos\n",
    "- Utiliza listas con `-` o `1.`\n",
    "- Emplea `**texto**` para enfatizar conceptos importantes\n",
    "- Utiliza `>` para citas textuales de la base de conocimientos\n",
    "- Implementa tablas cuando sea apropiado para presentar datos estructurados\n",
    "\n",
    "Recuerda: \n",
    "- Tu valor reside en proporcionar ÚNICAMENTE información verificada que exista en tu base de conocimientos, sin añadir especulaciones ni conocimientos externos.\n",
    "\n",
    "Solicitud del usuario: {query}\n",
    "\n",
    "Base de conocimientos: {context}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de respuesta a través de LLM (Local)\n",
    "\n",
    "Este componente final utiliza un Modelo de Lenguaje Grande (LLM) local para generar respuestas naturales basadas en la información recuperada.\n",
    "\n",
    "El proceso funciona así:\n",
    "\n",
    "- **Respuestas contextualizadas**:\n",
    "  * El modelo usa específicamente los fragmentos relevantes encontrados\n",
    "  * Mantiene la respuesta enfocada en la información proporcionada\n",
    "\n",
    "- **Se adapta a modelos locales**:\n",
    "  * Funciona con modelos de lenguaje ejecutados en el servidor local\n",
    "  * Permite ajustar parámetros como la \"temperatura\" (qué tan creativo o conservador será el modelo)\n",
    "\n",
    "Este componente es como un escritor experto que estudia los fragmentos seleccionados y redacta una respuesta coherente y natural, mostrándote su trabajo mientras lo hace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = answer_question_with_context_streaming(\n",
    "    query= query, context_docs= results, api_key= \"123\",\n",
    "    base_url= \"http://localhost:3000/v1\", model= \"gpt-4o\",\n",
    "    prompt_template=prompt\n",
    ")\n",
    "\n",
    "output = \"\"\n",
    "for token in stream:\n",
    "    output += token\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Convertir Markdown a HTML\n",
    "    html_content = markdown.markdown(output)\n",
    "    \n",
    "    # Aplicar estilo al HTML\n",
    "    styled_html = f\"\"\"\n",
    "    <div style=\"font-size: 18px; line-height: 1.5;\">\n",
    "        {html_content}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(styled_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
